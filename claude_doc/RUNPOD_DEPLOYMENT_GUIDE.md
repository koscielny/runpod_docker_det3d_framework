# RunPod éƒ¨ç½²å®Œæ•´æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—æä¾›ä»è´­ä¹°RunPodå®ä¾‹åˆ°æˆåŠŸéƒ¨ç½²å¤šæ¨¡å‹è¯„æµ‹æ¡†æ¶çš„å®Œæ•´æ­¥éª¤æ¸…å•ã€‚é€‚ç”¨äºåœ¨RunPodäº‘å¹³å°ä¸Šéƒ¨ç½²MapTRã€PETRã€StreamPETRã€TopoMLPã€VADç­‰3Dæ£€æµ‹å’Œåœ°å›¾æ„å»ºæ¨¡å‹ã€‚

## ğŸ›’ ç¬¬ä¸€æ­¥ï¼šè´­ä¹°å’Œè®¾ç½® RunPod

### 1.1 æ³¨å†Œè´¦æˆ·
```bash
# è®¿é—® https://runpod.io
# æ³¨å†Œè´¦æˆ·å¹¶å®Œæˆé‚®ç®±éªŒè¯
# æ·»åŠ ä»˜æ¬¾æ–¹å¼ï¼ˆä¿¡ç”¨å¡æˆ–å……å€¼ä½™é¢ï¼‰
```

### 1.2 é€‰æ‹©Pod Templateï¼ˆæ¨èé…ç½®ï¼‰

**ğŸ”¥ å¼ºçƒˆæ¨èæ¨¡æ¿ï¼š**

1. **PyTorch 2.1** (æœ€å…¼å®¹)
   - åŸºç¡€é•œåƒï¼š`runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04`
   - é¢„è£…ï¼šPyTorch 2.1, CUDA 11.8, Python 3.10
   - ä¼˜ç‚¹ï¼šç‰ˆæœ¬è¾ƒæ–°ï¼Œå…¼å®¹æ€§å¥½ï¼Œæ€§èƒ½ä¼˜ç§€
   - âš ï¸ éœ€è¦ï¼šéœ€è¦è°ƒæ•´æˆ‘ä»¬çš„DockerfileåŸºç¡€é•œåƒ

2. **PyTorch 1.13** (å®Œç¾åŒ¹é…) â­
   - åŸºç¡€é•œåƒï¼š`runpod/pytorch:1.13.0-py3.10-cuda11.7.1-devel`
   - é¢„è£…ï¼šPyTorch 1.13, CUDA 11.7, Python 3.10
   - ä¼˜ç‚¹ï¼šä¸æˆ‘ä»¬çš„CUDA 11.1æ¥è¿‘ï¼Œå…¼å®¹æ€§å¥½
   - æ¨èç†ç”±ï¼šç‰ˆæœ¬æœ€æ¥è¿‘æˆ‘ä»¬çš„éœ€æ±‚

3. **Universal Template** (çµæ´»é€‰æ‹©)
   - åŸºç¡€é•œåƒï¼š`runpod/base:0.4.0-cuda11.8.0`
   - é¢„è£…ï¼šåŸºç¡€CUDAç¯å¢ƒ
   - ä¼˜ç‚¹ï¼šæœ€å¤§çµæ´»æ€§ï¼Œå¯ä»¥å®Œå…¨æŒ‰éœ€é…ç½®
   - é€‚åˆï¼šéœ€è¦å®Œå…¨æ§åˆ¶ç¯å¢ƒçš„é«˜çº§ç”¨æˆ·

**ğŸ’° GPUå®ä¾‹æ¨èï¼š**

| GPUå‹å· | æ˜¾å­˜ | ä»·æ ¼/å°æ—¶ | æ¨èç”¨é€” | æ€§ä»·æ¯” |
|---------|------|----------|----------|--------|
| **RTX A5000** | 24GB | ~$0.50 | ğŸ”¥æ¨è-å®Œæ•´è¯„æµ‹ | â­â­â­â­â­ |
| RTX 4090 | 24GB | ~$0.70 | é«˜æ€§èƒ½è¯„æµ‹ | â­â­â­â­ |
| RTX 3090 | 24GB | ~$0.45 | é¢„ç®—å‹å¥½ | â­â­â­â­â­ |
| RTX A6000 | 48GB | ~$0.80 | å¤§è§„æ¨¡è¯„æµ‹ | â­â­â­ |
| RTX 4080 | 16GB | ~$0.40 | å•æ¨¡å‹æµ‹è¯• | â­â­â­ |

### 1.3 å¯åŠ¨å®ä¾‹é…ç½®
```bash
# åœ¨RunPodæ§åˆ¶å°è®¾ç½®ï¼š
# 1. é€‰æ‹© "GPU Pods"
# 2. é€‰æ‹©æ¨èçš„Pod Template
# 3. é…ç½®å­˜å‚¨ç©ºé—´ (æ¨è 150GB+)
# 4. è®¾ç½®å®¹å™¨ç£ç›˜ç©ºé—´ (50GB+)
# 5. è®¾ç½®ç«¯å£æ˜ å°„ (SSH: 22, HTTP: 8080)
# 6. ç‚¹å‡» "Deploy" å¯åŠ¨å®ä¾‹
```

**å­˜å‚¨é…ç½®å»ºè®®ï¼š**
- **å®¹å™¨ç£ç›˜**: 50GB (Dockeré•œåƒå’Œæ¨¡å‹)
- **å·å­˜å‚¨**: 100GB (æ•°æ®é›†å’Œç»“æœ)
- **ç½‘ç»œå­˜å‚¨**: å¯é€‰ï¼Œç”¨äºæŒä¹…åŒ–

## ğŸ”§ ç¬¬äºŒæ­¥ï¼šè¿æ¥å’Œåˆå§‹åŒ–

### 2.1 SSHè¿æ¥è®¾ç½®
```bash
# æ–¹æ³•1ï¼šä½¿ç”¨RunPodæä¾›çš„SSHå‘½ä»¤
ssh root@<pod-id>-<port>.proxy.runpod.net -p <port>

# æ–¹æ³•2ï¼šä½¿ç”¨Web Terminal (åœ¨æµè§ˆå™¨ä¸­)
# ç‚¹å‡»å®ä¾‹çš„ "Connect" -> "Start Web Terminal"

# æ–¹æ³•3ï¼šä½¿ç”¨Jupyter Lab
# ç‚¹å‡»å®ä¾‹çš„ "Connect" -> "Connect to Jupyter Lab"
```

### 2.2 ç³»ç»Ÿåˆå§‹åŒ–
```bash
# æ›´æ–°ç³»ç»ŸåŒ…
apt update && apt upgrade -y

# å®‰è£…å¿…è¦å·¥å…·
apt install -y git wget curl htop tree unzip

# å®‰è£…GPUç›‘æ§å·¥å…·
apt install -y nvtop

# éªŒè¯GPUå’ŒCUDA
nvidia-smi
nvcc --version
```

### 2.3 éªŒè¯ç¯å¢ƒ
```bash
# æ£€æŸ¥Pythonå’ŒPyTorch
python3 --version
python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}')"

# æ£€æŸ¥Docker
docker --version
docker ps
```

## ğŸ“¦ ç¬¬ä¸‰æ­¥ï¼šéƒ¨ç½²è¯„æµ‹æ¡†æ¶

### 3.1 å…‹éš†é¡¹ç›®
```bash
# åˆ‡æ¢åˆ°å·¥ä½œç›®å½•
cd /workspace  # RunPodé»˜è®¤å·¥ä½œç›®å½•

# å…‹éš†è¯„æµ‹æ¡†æ¶
git clone https://github.com/yilan-slam/runpod_docker_det3d_framework.git
cd runpod_docker_det3d_framework

# éªŒè¯æ–‡ä»¶ç»“æ„
tree -L 2
```

### 3.2 ç³»ç»ŸéªŒè¯
```bash
# è®¾ç½®æ‰§è¡Œæƒé™
chmod +x *.sh
chmod +x claude_doc/*.sh

# è¿è¡Œå¿«é€Ÿæµ‹è¯•
./quick_test.sh

# æœŸæœ›è¾“å‡ºï¼š
# âœ… è„šæœ¬è¯­æ³•æ£€æŸ¥é€šè¿‡
# âœ… Pythonæ¨¡å—å¯¼å…¥æˆåŠŸ  
# âœ… è¾“å‡ºæ ‡å‡†åŒ–æµ‹è¯•é€šè¿‡
# âœ… å¥åº·æ£€æŸ¥æµ‹è¯•é€šè¿‡
```

### 3.3 ç¯å¢ƒé…ç½®ï¼ˆå¦‚æœéœ€è¦ï¼‰
```bash
# å¦‚æœä½¿ç”¨PyTorch 2.1æ¨¡æ¿ï¼Œéœ€è¦è°ƒæ•´Dockerfile
# ç¼–è¾‘æ‰€æœ‰æ¨¡å‹çš„Dockerfileï¼Œå°†åŸºç¡€é•œåƒæ”¹ä¸ºï¼š
# FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel

# æ›´æ–°requirements.txtä¸­çš„PyTorchç‰ˆæœ¬ï¼š
# torch==2.1.0
# torchvision==0.16.0

# å¦‚æœä½¿ç”¨Universalæ¨¡æ¿ï¼Œéœ€è¦å®‰è£…Pythonå’ŒåŸºç¡€åŒ…
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ³ ç¬¬å››æ­¥ï¼šæ„å»ºDockeré•œåƒ

### 4.1 æ„å»ºæ‰€æœ‰æ¨¡å‹é•œåƒ
```bash
# æ„å»ºæ‰€æœ‰æ¨¡å‹çš„Dockeré•œåƒ
./build_model_image.sh

# æ„å»ºè¿‡ç¨‹çº¦30-60åˆ†é’Ÿï¼Œä¾æ¬¡æ„å»ºï¼š
# 1. MapTR (HDåœ°å›¾æ„å»º)
# 2. PETR (å¤šè§†è§’3Dæ£€æµ‹)
# 3. StreamPETR (æ—¶åº3Dæ£€æµ‹)  
# 4. TopoMLP (æ‹“æ‰‘æ¨ç†)
# 5. VAD (çŸ¢é‡åŒ–è‡ªåŠ¨é©¾é©¶)
```

### 4.2 éªŒè¯é•œåƒæ„å»º
```bash
# æ£€æŸ¥æ„å»ºçš„é•œåƒ
docker images | grep -E "(maptr|petr|streampetr|topomlp|vad)"

# æœŸæœ›è¾“å‡ºï¼šæ¯ä¸ªé•œåƒçº¦5-8GB
# maptr-runpod    latest    abc123    5.2GB
# petr-runpod     latest    def456    4.8GB
# streampetr-runpod latest  ghi789    5.1GB
# topomlp-runpod  latest    jkl012    4.5GB
# vad-runpod      latest    mno345    5.5GB
```

### 4.3 ç£ç›˜ç©ºé—´æ£€æŸ¥
```bash
# æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ
df -h

# å¦‚æœç©ºé—´ä¸è¶³ï¼Œæ¸…ç†Dockerç¼“å­˜
docker system prune -a -f

# æ£€æŸ¥Dockeré•œåƒå¤§å°
docker images --format "{{.Repository}}:{{.Tag}} {{.Size}}"
```

## ğŸ¥ ç¬¬äº”æ­¥ï¼šå¥åº·æ£€æŸ¥

### 5.1 ç³»ç»Ÿå¥åº·éªŒè¯
```bash
# è¿è¡Œå®Œæ•´å¥åº·æ£€æŸ¥
./run_model_evaluation.sh --health-check

# æœŸæœ›è¾“å‡ºï¼š
# ğŸ¥ ç³»ç»Ÿå¥åº·æ£€æŸ¥
# âœ… MapTR: å¥åº·
# âœ… PETR: å¥åº·  
# âœ… StreamPETR: å¥åº·
# âœ… TopoMLP: å¥åº·
# âœ… VAD: å¥åº·
```

### 5.2 å•ç‹¬æ£€æŸ¥æ¯ä¸ªæ¨¡å‹
```bash
# æ£€æŸ¥å„ä¸ªæ¨¡å‹çš„å¥åº·çŠ¶æ€
python claude_doc/health_check.py --model MapTR --mode check
python claude_doc/health_check.py --model PETR --mode check
python claude_doc/health_check.py --model StreamPETR --mode check
python claude_doc/health_check.py --model TopoMLP --mode check
python claude_doc/health_check.py --model VAD --mode check
```

### 5.3 é…ç½®éªŒè¯
```bash
# åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®
./list_model_configs.sh

# éªŒè¯é…ç½®æ–‡ä»¶å®Œæ•´æ€§
python validate_config.py --help

# æµ‹è¯•é…ç½®éªŒè¯
python validate_config.py --config /app/MapTR/projects/configs/maptr/maptr_tiny_r50_24e.py --model MapTR
```

## ğŸ“Š ç¬¬å…­æ­¥ï¼šè¿è¡Œæµ‹è¯•è¯„ä¼°

### 6.1 å‡†å¤‡æµ‹è¯•æ•°æ®
```bash
# åˆ›å»ºæµ‹è¯•æ•°æ®ç›®å½•
mkdir -p /workspace/test_data

# åˆ›å»ºæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
echo "sample_test_token_001" > /workspace/test_data/sample.txt
echo "sample_test_token_002" > /workspace/test_data/sample2.txt

# åˆ›å»ºæµ‹è¯•é…ç½®
export DATA_PATH="/workspace/test_data/sample.txt"
```

### 6.2 å•æ¨¡å‹æµ‹è¯•
```bash
# æµ‹è¯•MapTR (HDåœ°å›¾æ„å»º)
./run_model_evaluation.sh --single-model MapTR --data-path $DATA_PATH

# æµ‹è¯•PETR (3Dæ£€æµ‹)
./run_model_evaluation.sh --single-model PETR --data-path $DATA_PATH

# æµ‹è¯•StreamPETR (æ—¶åºæ£€æµ‹)
./run_model_evaluation.sh --single-model StreamPETR --data-path $DATA_PATH

# æµ‹è¯•TopoMLP (æ‹“æ‰‘æ¨ç†)
./run_model_evaluation.sh --single-model TopoMLP --data-path $DATA_PATH

# æµ‹è¯•VAD (çŸ¢é‡åŒ–é©¾é©¶)
./run_model_evaluation.sh --single-model VAD --data-path $DATA_PATH
```

### 6.3 å¤šæ¨¡å‹æ¯”è¾ƒè¯„æµ‹
```bash
# è¿è¡Œå®Œæ•´è¯„æµ‹ (æ¨è)
./run_model_evaluation.sh --full-evaluation

# æ¯”è¾ƒç‰¹å®šæ¨¡å‹ç»„åˆ
./run_model_evaluation.sh --compare-models --models MapTR,PETR,VAD

# å¿«é€Ÿæ¯”è¾ƒ (ä»…æ£€æµ‹æ¨¡å‹)
./run_model_evaluation.sh --compare-models --models PETR,StreamPETR,TopoMLP
```

### 6.4 æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
# è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
./run_model_evaluation.sh --benchmark --iterations 10

# å‹åŠ›æµ‹è¯•
./run_model_evaluation.sh --stress-test --duration 30m
```

## ğŸ“ˆ ç¬¬ä¸ƒæ­¥ï¼šæŸ¥çœ‹å’Œåˆ†æç»“æœ

### 7.1 æ£€æŸ¥è¯„æµ‹ç»“æœ
```bash
# æŸ¥çœ‹ç”Ÿæˆçš„ç»“æœç›®å½•
ls -la evaluation_results/

# å…¸å‹ç»“æœç»“æ„ï¼š
# evaluation_results/
# â”œâ”€â”€ comparison_report.json       # æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š
# â”œâ”€â”€ individual_results/          # å•æ¨¡å‹ç»“æœ
# â”œâ”€â”€ visualizations/              # å¯è§†åŒ–å›¾è¡¨
# â”œâ”€â”€ performance_metrics.json     # æ€§èƒ½æŒ‡æ ‡
# â””â”€â”€ system_info.json            # ç³»ç»Ÿä¿¡æ¯
```

### 7.2 æŸ¥çœ‹æ¯”è¾ƒæŠ¥å‘Š
```bash
# æŸ¥çœ‹JSONæ ¼å¼æŠ¥å‘Š
cat evaluation_results/comparison_report.json | python -m json.tool

# æŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡
cat evaluation_results/performance_metrics.json | python -m json.tool

# æŸ¥çœ‹å¯è§†åŒ–æ–‡ä»¶
ls evaluation_results/visualizations/
# åº”è¯¥åŒ…å«ï¼š
# - radar_chart.png (é›·è¾¾å›¾)
# - performance_bar_chart.png (æŸ±çŠ¶å›¾)
# - memory_usage_chart.png (å†…å­˜ä½¿ç”¨å›¾)
```

### 7.3 åˆ†æç»“æœç¤ºä¾‹
```bash
# æå–å…³é”®æ€§èƒ½æŒ‡æ ‡
python3 -c "
import json
with open('evaluation_results/comparison_report.json') as f:
    data = json.load(f)
    
print('=== æ¨¡å‹æ€§èƒ½æ’å ===')
ranking = data['performance_ranking']
for metric, models in ranking.items():
    print(f'{metric}: {models[0]}')
"
```

## ğŸ”§ ç¬¬å…«æ­¥ï¼šé«˜çº§åŠŸèƒ½éªŒè¯

### 8.1 SSHå¼€å‘ç¯å¢ƒæµ‹è¯•
```bash
# å¯åŠ¨SSHæœåŠ¡å™¨
./start_ssh_server.sh

# éªŒè¯SSHè¿æ¥ï¼ˆåœ¨æœ¬åœ°æœºå™¨ä¸Šï¼‰
# ssh runpod@<runpod-ip> -p 22
```

### 8.2 æ•°æ®é›†ç®¡ç†æµ‹è¯•
```bash
# è¿›å…¥æ•°æ®é›†ç®¡ç†ç›®å½•
cd claude_doc

# æŸ¥çœ‹æ•°æ®é›†ä¸‹è½½æŒ‡å—
cat DATASET_DOWNLOAD_GUIDE.md

# æµ‹è¯•æ•°æ®é›†è„šæœ¬ï¼ˆä¸å®é™…ä¸‹è½½ï¼‰
./download_datasets.sh --help
```

### 8.3 å¥åº·ç›‘æ§HTTPç«¯ç‚¹
```bash
# å¯åŠ¨å¥åº·æ£€æŸ¥HTTPæœåŠ¡å™¨
python claude_doc/health_check.py --mode server --port 8080 &

# æµ‹è¯•HTTPç«¯ç‚¹
curl http://localhost:8080/health

# åœ¨RunPodå¤–éƒ¨è®¿é—®ï¼ˆä½¿ç”¨å…¬å¼€ç«¯å£ï¼‰
# curl http://<runpod-public-ip>:8080/health
```

## ğŸ” æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è§é—®é¢˜1ï¼šDockeræ„å»ºå¤±è´¥
```bash
# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# å¦‚æœç©ºé—´ä¸è¶³
docker system prune -a -f
apt autoremove -y

# æ£€æŸ¥ç½‘ç»œè¿æ¥
ping github.com

# é‡æ–°æ„å»ºç‰¹å®šæ¨¡å‹
docker build -t maptr-runpod ./MapTR/
```

### å¸¸è§é—®é¢˜2ï¼šGPUå†…å­˜ä¸è¶³
```bash
# æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
nvidia-smi

# æ¸…ç†GPUå†…å­˜
python3 -c "import torch; torch.cuda.empty_cache()"

# é‡å¯DockeræœåŠ¡
sudo systemctl restart docker

# é™ä½æ‰¹å¤„ç†å¤§å°
export BATCH_SIZE=1
```

### å¸¸è§é—®é¢˜3ï¼šæ¨¡å‹æ¨ç†å¤±è´¥
```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æƒé™
ls -la /workspace/models/

# æ£€æŸ¥Dockerå®¹å™¨æ—¥å¿—
docker logs <container_name>

# é‡æ–°è¿è¡Œå¥åº·æ£€æŸ¥
./run_model_evaluation.sh --health-check

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
./run_model_evaluation.sh --single-model MapTR --data-path $DATA_PATH --verbose
```

### å¸¸è§é—®é¢˜4ï¼šç½‘ç»œå’Œè¿æ¥é—®é¢˜
```bash
# æ£€æŸ¥RunPodç«¯å£é…ç½®
# ç¡®ä¿åœ¨RunPodæ§åˆ¶å°ä¸­å¼€æ”¾äº†å¿…è¦ç«¯å£ï¼š22 (SSH), 8080 (HTTP)

# æ£€æŸ¥é˜²ç«å¢™
ufw status

# é‡å¯ç½‘ç»œæœåŠ¡
systemctl restart networking
```

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### GPUä¼˜åŒ–
```bash
# è®¾ç½®CUDAç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0
export TORCH_CUDA_ARCH_LIST="8.6"  # RTX A5000
export CUDA_LAUNCH_BLOCKING=0  # å¼‚æ­¥æ‰§è¡Œ

# GPUå†…å­˜ç®¡ç†
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### å¹¶è¡ŒåŒ–è®¾ç½®
```bash
# å¤šè¿›ç¨‹æ¨ç†
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8

# å¹¶è¡Œæ¨¡å‹æ¯”è¾ƒ
./run_model_evaluation.sh --parallel --workers 2
```

### ç¼“å­˜ä¼˜åŒ–
```bash
# å¯ç”¨Dockerå±‚ç¼“å­˜
export DOCKER_BUILDKIT=1

# è®¾ç½®PyTorchç¼“å­˜
export TORCH_HOME=/workspace/.torch
mkdir -p $TORCH_HOME
```

## ğŸ“‹ éƒ¨ç½²æˆåŠŸæ£€æŸ¥æ¸…å•

### âœ… éƒ¨ç½²å‰æ£€æŸ¥
- [ ] RunPodå®ä¾‹æ­£å¸¸å¯åŠ¨ (GPU: RTX A5000/4090/3090)
- [ ] SSHè¿æ¥æˆåŠŸå»ºç«‹
- [ ] GPUé©±åŠ¨å’ŒCUDAæ­£å¸¸å·¥ä½œ (`nvidia-smi`)
- [ ] DockeræœåŠ¡è¿è¡Œæ­£å¸¸ (`docker ps`)
- [ ] ç½‘ç»œè¿æ¥ç¨³å®š (`ping github.com`)
- [ ] ç£ç›˜ç©ºé—´å……è¶³ (>100GBå¯ç”¨)

### âœ… æ„å»ºéªŒè¯
- [ ] é¡¹ç›®ä»£ç æˆåŠŸå…‹éš†
- [ ] `./quick_test.sh` å…¨éƒ¨é€šè¿‡
- [ ] æ‰€æœ‰5ä¸ªDockeré•œåƒæ„å»ºæˆåŠŸ
- [ ] é•œåƒå¤§å°åˆç† (æ¯ä¸ª4-8GB)
- [ ] æ— æ„å»ºé”™è¯¯æˆ–è­¦å‘Š

### âœ… åŠŸèƒ½éªŒè¯  
- [ ] å¥åº·æ£€æŸ¥å…¨éƒ¨é€šè¿‡ (`--health-check`)
- [ ] è‡³å°‘3ä¸ªæ¨¡å‹èƒ½æˆåŠŸæ¨ç†
- [ ] å¤šæ¨¡å‹æ¯”è¾ƒèƒ½æ­£å¸¸è¿è¡Œ
- [ ] ç”Ÿæˆå¯è§†åŒ–ç»“æœæ–‡ä»¶
- [ ] HTTPå¥åº·ç«¯ç‚¹å“åº”æ­£å¸¸

### âœ… æ€§èƒ½éªŒè¯
- [ ] å•æ¨¡å‹æ¨ç†æ—¶é—´ <1ç§’
- [ ] GPUå†…å­˜ä½¿ç”¨åˆç† (<20GB)
- [ ] ç³»ç»Ÿèµ„æºä½¿ç”¨æ­£å¸¸
- [ ] æ²¡æœ‰å†…å­˜æ³„æ¼é—®é¢˜

## ğŸ’° è´¹ç”¨æ§åˆ¶å’Œé¢„ç®—

### é¢„ç®—ä¼°ç®—
```bash
# å¼€å‘æµ‹è¯•é˜¶æ®µ (RTX A5000 @ $0.50/hour)
# - åˆå§‹éƒ¨ç½²å’Œæµ‹è¯•: 2-4å°æ—¶ = $1-2
# - åŠŸèƒ½éªŒè¯: 2-3å°æ—¶ = $1-1.5  
# - æ€§èƒ½è°ƒä¼˜: 3-5å°æ—¶ = $1.5-2.5
# æ€»è®¡: $3.5-6

# å®Œæ•´è¯„æµ‹é˜¶æ®µ
# - å…¨æ¨¡å‹è¯„æµ‹: 4-6å°æ—¶ = $2-3
# - æ•°æ®é›†æµ‹è¯•: 6-10å°æ—¶ = $3-5
# - æŠ¥å‘Šç”Ÿæˆ: 1-2å°æ—¶ = $0.5-1
# æ€»è®¡: $5.5-9

# å»ºè®®é¢„ç®—: $15-20 (åŒ…å«è°ƒè¯•å’Œé‡è¯•æ—¶é—´)
```

### èŠ‚çœè´¹ç”¨æŠ€å·§
```bash
# 1. åŠæ—¶æš‚åœå®ä¾‹
# å®Œæˆæµ‹è¯•ååœ¨RunPodæ§åˆ¶å°ç‚¹å‡» "Stop"
# æ•°æ®ä¼šä¿ç•™åœ¨å·å­˜å‚¨ä¸­ï¼Œåªåœæ­¢GPUè®¡è´¹

# 2. æ‰¹é‡å¤„ç†ä»»åŠ¡
./run_model_evaluation.sh --full-evaluation --batch-mode

# 3. ä½¿ç”¨spotå®ä¾‹
# åœ¨RunPodä¸­é€‰æ‹© "Spot" å®ä¾‹ï¼Œä»·æ ¼å¯ä½30-50%

# 4. é€‰æ‹©åˆé€‚çš„GPU
# RTX 3090 (24GB): æœ€ä¾¿å®œï¼Œæ€§èƒ½è¶³å¤Ÿ
# RTX A5000 (24GB): æ€§ä»·æ¯”æœ€å¥½
# RTX 4090 (24GB): æ€§èƒ½æœ€å¼ºï¼Œä»·æ ¼è¾ƒé«˜
```

## ğŸ¯ éƒ¨ç½²æˆåŠŸæ ‡å¿—

å½“ä½ çœ‹åˆ°ä»¥ä¸‹å®Œæ•´è¾“å‡ºæ—¶ï¼Œè¯´æ˜éƒ¨ç½²å®Œå…¨æˆåŠŸï¼š

```bash
ğŸ‰ RunPodå¤šæ¨¡å‹è¯„æµ‹ç³»ç»Ÿéƒ¨ç½²æˆåŠŸï¼

ğŸ“Š ç³»ç»ŸçŠ¶æ€:
  âœ… GPU: RTX A5000 (24GB) - æ­£å¸¸å·¥ä½œ
  âœ… CUDA: 11.8 - å…¼å®¹
  âœ… Docker: 5ä¸ªæ¨¡å‹é•œåƒæ„å»ºæˆåŠŸ
  âœ… å¥åº·æ£€æŸ¥: å…¨éƒ¨é€šè¿‡

ğŸ“‹ å¯ç”¨åŠŸèƒ½:
  âœ… æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ - ç»Ÿä¸€æ‰€æœ‰æ¨¡å‹è¾“å‡º
  âœ… å¥åº·æ£€æŸ¥ç«¯ç‚¹ - éªŒè¯æ¨¡å‹å’Œç³»ç»ŸçŠ¶æ€  
  âœ… æ¨¡å‹æ¯”è¾ƒåˆ†æ - å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”
  âœ… é…ç½®åŠ¨æ€ç®¡ç† - çµæ´»çš„é…ç½®æ–‡ä»¶æ”¯æŒ
  âœ… SSHå¼€å‘ç¯å¢ƒ - VS Codeè¿œç¨‹å¼€å‘
  âœ… æ•°æ®é›†ç®¡ç† - è‡ªåŠ¨åŒ–ä¸‹è½½å’ŒéªŒè¯

ğŸš€ å¿«é€Ÿå¼€å§‹å‘½ä»¤:
  ./run_model_evaluation.sh --health-check
  ./run_model_evaluation.sh --full-evaluation
  ./run_model_evaluation.sh --compare-models --models MapTR,PETR,VAD

ğŸ“ˆ é¢„æœŸæ€§èƒ½ (RTX A5000):
  MapTR: ~0.20s, 2.0GB VRAM
  PETR: ~0.15s, 1.7GB VRAM  
  StreamPETR: ~0.18s, 1.9GB VRAM
  TopoMLP: ~0.12s, 1.4GB VRAM
  VAD: ~0.25s, 2.2GB VRAM

ğŸ¯ ç³»ç»Ÿå·²å®Œå…¨å°±ç»ªï¼Œå¼€å§‹ä½ çš„å¤šæ¨¡å‹è¯„æµ‹ä¹‹æ—…ï¼
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### æ•…éšœè¯Šæ–­
1. é¦–å…ˆè¿è¡Œ `./quick_test.sh` è¿›è¡Œç³»ç»Ÿè‡ªæ£€
2. æŸ¥çœ‹ `claude_doc/TODO.md` äº†è§£å·²çŸ¥é—®é¢˜
3. è¿è¡Œ `./run_model_evaluation.sh --health-check` è·å–è¯¦ç»†çŠ¶æ€

### æ–‡æ¡£èµ„æº
- `claude_doc/EVALUATION_GUIDE.md`: å®Œæ•´è¯„æµ‹å·¥ä½œæµç¨‹
- `claude_doc/DATASET_DOWNLOAD_GUIDE.md`: æ•°æ®é›†è®¾ç½®è¯´æ˜  
- `claude_doc/IMPLEMENTATION_DETAILS.md`: æŠ€æœ¯å®ç°è¯¦æƒ…
- `SYSTEM_SUMMARY.md`: é¡¹ç›®å®Œæˆæ€»ç»“

### ç¤¾åŒºæ”¯æŒ
- GitHub Issues: æŠ¥å‘Šé—®é¢˜å’Œè·å–å¸®åŠ©
- RunPod Discord: RunPodå¹³å°ç›¸å…³é—®é¢˜
- é¡¹ç›®æ–‡æ¡£: å®Œæ•´çš„ä½¿ç”¨å’Œå¼€å‘æŒ‡å—

---

é€šè¿‡éµå¾ªè¿™ä¸ªè¯¦ç»†çš„éƒ¨ç½²æŒ‡å—ï¼Œä½ åº”è¯¥èƒ½å¤Ÿåœ¨RunPodä¸ŠæˆåŠŸéƒ¨ç½²å¹¶è¿è¡Œå®Œæ•´çš„å¤šæ¨¡å‹3Dæ£€æµ‹å’Œåœ°å›¾æ„å»ºè¯„æµ‹ç³»ç»Ÿï¼