#!/usr/bin/env python3
"""
å¤šæ¨¡å‹æ¯”è¾ƒå’Œåˆ†æå·¥å…·
ç”¨äºæ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨ç›¸åŒæ•°æ®ä¸Šçš„è¡¨ç°
"""

import json
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from model_output_standard import StandardOutput

@dataclass
class ModelPerformance:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    model_name: str
    inference_time: float      # æ¨ç†æ—¶é—´ (ç§’)
    gpu_memory_used: float     # GPUå†…å­˜ä½¿ç”¨ (MB)
    detection_count: int       # æ£€æµ‹ç›®æ ‡æ•°é‡
    map_element_count: int     # åœ°å›¾å…ƒç´ æ•°é‡
    avg_confidence: float      # å¹³å‡ç½®ä¿¡åº¦
    high_conf_ratio: float     # é«˜ç½®ä¿¡åº¦æ¯”ä¾‹ (>0.7)
    error_status: Optional[str] # é”™è¯¯çŠ¶æ€

class ModelComparator:
    """å¤šæ¨¡å‹æ¯”è¾ƒå™¨"""
    
    def __init__(self, output_dir: str = "./comparison_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.results: List[StandardOutput] = []
        self.performances: List[ModelPerformance] = []
    
    def add_result(self, result: StandardOutput):
        """æ·»åŠ æ¨¡å‹ç»“æœ"""
        self.results.append(result)
        self._calculate_performance(result)
    
    def _calculate_performance(self, result: StandardOutput) -> ModelPerformance:
        """è®¡ç®—æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
        # åŸºç¡€æŒ‡æ ‡
        inference_time = result.metadata.inference_time
        gpu_memory = result.metadata.gpu_memory_used
        
        # æ£€æµ‹ç»Ÿè®¡
        detection_count = len(result.detections_3d) if result.detections_3d else 0
        map_element_count = len(result.map_elements) if result.map_elements else 0
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        confidences = []
        if result.detections_3d:
            confidences.extend([det.confidence for det in result.detections_3d])
        if result.map_elements:
            confidences.extend([elem.confidence for elem in result.map_elements])
        
        avg_confidence = np.mean(confidences) if confidences else 0.0
        high_conf_ratio = sum(1 for c in confidences if c > 0.7) / len(confidences) if confidences else 0.0
        
        performance = ModelPerformance(
            model_name=result.metadata.model_name,
            inference_time=inference_time,
            gpu_memory_used=gpu_memory,
            detection_count=detection_count,
            map_element_count=map_element_count,
            avg_confidence=avg_confidence,
            high_conf_ratio=high_conf_ratio,
            error_status=result.error
        )
        
        self.performances.append(performance)
        return performance
    
    def generate_comparison_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        if not self.performances:
            return {"error": "No results to compare"}
        
        # åˆ›å»ºæ€§èƒ½æ•°æ®æ¡†
        df = pd.DataFrame([
            {
                'Model': p.model_name,
                'Inference_Time_s': p.inference_time,
                'GPU_Memory_MB': p.gpu_memory_used,
                'Detection_Count': p.detection_count,
                'Map_Element_Count': p.map_element_count,
                'Avg_Confidence': p.avg_confidence,
                'High_Conf_Ratio': p.high_conf_ratio,
                'Has_Error': p.error_status is not None
            }
            for p in self.performances
        ])
        
        # ç»Ÿè®¡åˆ†æ
        report = {
            "summary": {
                "total_models": len(self.performances),
                "successful_models": sum(1 for p in self.performances if p.error_status is None),
                "failed_models": sum(1 for p in self.performances if p.error_status is not None)
            },
            "performance_ranking": {},
            "detailed_comparison": df.to_dict('records'),
            "insights": []
        }
        
        # æ€§èƒ½æ’å
        if len(df) > 1:
            report["performance_ranking"] = {
                "fastest_inference": df.loc[df['Inference_Time_s'].idxmin(), 'Model'],
                "lowest_memory": df.loc[df['GPU_Memory_MB'].idxmin(), 'Model'],
                "most_detections": df.loc[df['Detection_Count'].idxmax(), 'Model'],
                "highest_confidence": df.loc[df['Avg_Confidence'].idxmax(), 'Model']
            }
            
            # ç”Ÿæˆæ´å¯Ÿ
            report["insights"] = self._generate_insights(df)
        
        return report
    
    def _generate_insights(self, df: pd.DataFrame) -> List[str]:
        """ç”Ÿæˆåˆ†ææ´å¯Ÿ"""
        insights = []
        
        # æ¨ç†æ—¶é—´åˆ†æ
        time_range = df['Inference_Time_s'].max() - df['Inference_Time_s'].min()
        if time_range > 0.1:  # è¶…è¿‡100mså·®å¼‚
            fastest = df.loc[df['Inference_Time_s'].idxmin(), 'Model']
            slowest = df.loc[df['Inference_Time_s'].idxmax(), 'Model']
            speedup = df['Inference_Time_s'].max() / df['Inference_Time_s'].min()
            insights.append(f"æ¨ç†é€Ÿåº¦ï¼š{fastest} æ¯” {slowest} å¿« {speedup:.1f}x")
        
        # å†…å­˜ä½¿ç”¨åˆ†æ
        memory_range = df['GPU_Memory_MB'].max() - df['GPU_Memory_MB'].min()
        if memory_range > 500:  # è¶…è¿‡500MBå·®å¼‚
            efficient = df.loc[df['GPU_Memory_MB'].idxmin(), 'Model']
            hungry = df.loc[df['GPU_Memory_MB'].idxmax(), 'Model']
            insights.append(f"å†…å­˜æ•ˆç‡ï¼š{efficient} æ¯” {hungry} èŠ‚çœ {memory_range:.0f}MB GPUå†…å­˜")
        
        # æ£€æµ‹èƒ½åŠ›åˆ†æ
        if df['Detection_Count'].sum() > 0:
            best_detector = df.loc[df['Detection_Count'].idxmax(), 'Model']
            avg_detections = df['Detection_Count'].mean()
            insights.append(f"æ£€æµ‹èƒ½åŠ›ï¼š{best_detector} æ£€æµ‹åˆ°æœ€å¤šç›®æ ‡ï¼Œå¹³å‡æ£€æµ‹æ•°é‡ {avg_detections:.1f}")
        
        # ç½®ä¿¡åº¦åˆ†æ
        if df['Avg_Confidence'].max() > 0:
            most_confident = df.loc[df['Avg_Confidence'].idxmax(), 'Model']
            avg_conf = df['Avg_Confidence'].mean()
            insights.append(f"ç½®ä¿¡åº¦ï¼š{most_confident} å…·æœ‰æœ€é«˜å¹³å‡ç½®ä¿¡åº¦ï¼Œæ•´ä½“å¹³å‡ {avg_conf:.3f}")
        
        return insights
    
    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        if len(self.performances) < 2:
            print("éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹ç»“æœæ‰èƒ½åˆ›å»ºæ¯”è¾ƒå›¾è¡¨")
            return
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('å¤šæ¨¡å‹æ€§èƒ½æ¯”è¾ƒ', fontsize=16, fontweight='bold')
        
        # å‡†å¤‡æ•°æ®
        models = [p.model_name for p in self.performances]
        inference_times = [p.inference_time for p in self.performances]
        memory_usage = [p.gpu_memory_used for p in self.performances]
        detection_counts = [p.detection_count for p in self.performances]
        avg_confidences = [p.avg_confidence for p in self.performances]
        
        # 1. æ¨ç†æ—¶é—´æ¯”è¾ƒ
        axes[0, 0].bar(models, inference_times, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('æ¨ç†æ—¶é—´æ¯”è¾ƒ')
        axes[0, 0].set_ylabel('æ—¶é—´ (ç§’)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. GPUå†…å­˜ä½¿ç”¨æ¯”è¾ƒ
        axes[0, 1].bar(models, memory_usage, color='lightcoral', alpha=0.7)
        axes[0, 1].set_title('GPUå†…å­˜ä½¿ç”¨æ¯”è¾ƒ')
        axes[0, 1].set_ylabel('å†…å­˜ (MB)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. æ£€æµ‹æ•°é‡æ¯”è¾ƒ
        axes[1, 0].bar(models, detection_counts, color='lightgreen', alpha=0.7)
        axes[1, 0].set_title('æ£€æµ‹ç›®æ ‡æ•°é‡æ¯”è¾ƒ')
        axes[1, 0].set_ylabel('æ£€æµ‹æ•°é‡')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. å¹³å‡ç½®ä¿¡åº¦æ¯”è¾ƒ
        axes[1, 1].bar(models, avg_confidences, color='gold', alpha=0.7)
        axes[1, 1].set_title('å¹³å‡ç½®ä¿¡åº¦æ¯”è¾ƒ')
        axes[1, 1].set_ylabel('ç½®ä¿¡åº¦')
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_path = self.output_dir / "model_comparison_charts.png"
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜è‡³: {chart_path}")
        
        # åˆ›å»ºé›·è¾¾å›¾ (å¦‚æœæœ‰å¤šä¸ªæŒ‡æ ‡)
        self._create_radar_chart(models, inference_times, memory_usage, detection_counts, avg_confidences)
    
    def _create_radar_chart(self, models, inference_times, memory_usage, detection_counts, avg_confidences):
        """åˆ›å»ºé›·è¾¾å›¾æ¯”è¾ƒ"""
        try:
            from math import pi
            
            # æ ‡å‡†åŒ–æ•°æ® (0-1 èŒƒå›´ï¼Œå€¼è¶Šé«˜è¶Šå¥½)
            def normalize_metric(values, higher_better=True):
                if not values or max(values) == min(values):
                    return [0.5] * len(values)
                if higher_better:
                    return [(v - min(values)) / (max(values) - min(values)) for v in values]
                else:
                    return [(max(values) - v) / (max(values) - min(values)) for v in values]
            
            # æ ‡å‡†åŒ–æŒ‡æ ‡ (æ¨ç†æ—¶é—´å’Œå†…å­˜ä½¿ç”¨è¶Šä½è¶Šå¥½ï¼Œå…¶ä»–è¶Šé«˜è¶Šå¥½)
            norm_time = normalize_metric(inference_times, higher_better=False)
            norm_memory = normalize_metric(memory_usage, higher_better=False)
            norm_detections = normalize_metric(detection_counts, higher_better=True)
            norm_confidence = normalize_metric(avg_confidences, higher_better=True)
            
            # æŒ‡æ ‡åç§°
            metrics = ['æ¨ç†é€Ÿåº¦', 'GPUæ•ˆç‡', 'æ£€æµ‹èƒ½åŠ›', 'ç½®ä¿¡åº¦']
            
            # åˆ›å»ºé›·è¾¾å›¾
            fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
            
            # è§’åº¦
            angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]
            angles += angles[:1]  # å®Œæˆåœ†åœˆ
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶
            colors = ['red', 'blue', 'green', 'orange', 'purple']
            for i, model in enumerate(models):
                values = [norm_time[i], norm_memory[i], norm_detections[i], norm_confidence[i]]
                values += values[:1]  # å®Œæˆåœ†åœˆ
                
                ax.plot(angles, values, 'o-', linewidth=2, 
                       label=model, color=colors[i % len(colors)])
                ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
            
            # æ·»åŠ æ ‡ç­¾
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metrics)
            ax.set_ylim(0, 1)
            ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
            ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'])
            ax.grid(True)
            
            plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
            plt.title('æ¨¡å‹ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', size=16, fontweight='bold', pad=20)
            
            # ä¿å­˜é›·è¾¾å›¾
            radar_path = self.output_dir / "model_radar_comparison.png"
            plt.savefig(radar_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“ˆ é›·è¾¾å›¾å·²ä¿å­˜è‡³: {radar_path}")
            
        except Exception as e:
            print(f"åˆ›å»ºé›·è¾¾å›¾æ—¶å‡ºé”™: {e}")
    
    def save_results(self):
        """ä¿å­˜æ¯”è¾ƒç»“æœ"""
        # ä¿å­˜è¯¦ç»†çš„JSONç»“æœ
        detailed_results = {
            "models": [result.to_dict() for result in self.results],
            "performances": [
                {
                    "model_name": p.model_name,
                    "inference_time": p.inference_time,
                    "gpu_memory_used": p.gpu_memory_used,
                    "detection_count": p.detection_count,
                    "map_element_count": p.map_element_count,
                    "avg_confidence": p.avg_confidence,
                    "high_conf_ratio": p.high_conf_ratio,
                    "error_status": p.error_status
                }
                for p in self.performances
            ]
        }
        
        results_path = self.output_dir / "detailed_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ¯”è¾ƒæŠ¥å‘Š
        report = self.generate_comparison_report()
        report_path = self.output_dir / "comparison_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ ¼å¼çš„æ€§èƒ½æ•°æ®
        if self.performances:
            df = pd.DataFrame([
                {
                    'Model': p.model_name,
                    'Inference_Time_s': p.inference_time,
                    'GPU_Memory_MB': p.gpu_memory_used,
                    'Detection_Count': p.detection_count,
                    'Map_Element_Count': p.map_element_count,
                    'Avg_Confidence': p.avg_confidence,
                    'High_Conf_Ratio': p.high_conf_ratio,
                    'Has_Error': p.error_status is not None
                }
                for p in self.performances
            ])
            
            csv_path = self.output_dir / "performance_comparison.csv"
            df.to_csv(csv_path, index=False)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³ç›®å½•: {self.output_dir}")
        print(f"  - è¯¦ç»†ç»“æœ: {results_path}")
        print(f"  - æ¯”è¾ƒæŠ¥å‘Š: {report_path}")
        print(f"  - æ€§èƒ½CSV: {csv_path}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from model_output_standard import create_standardizer, ModelMetadata
    
    # åˆ›å»ºæ¯”è¾ƒå™¨
    comparator = ModelComparator("./model_comparison_demo")
    
    # æ¨¡æ‹Ÿä¸€äº›æµ‹è¯•ç»“æœ
    test_results = [
        {
            "model": "MapTR",
            "raw_output": [
                {"id": 0, "class_name": "divider", "confidence": 0.85, "pts": [[1, 2], [3, 4]]},
                {"id": 1, "class_name": "car", "confidence": 0.92, "bbox": [10, 10, 20, 20]}
            ],
            "metadata": {"inference_time": 0.25, "gpu_memory_used": 2048}
        },
        {
            "model": "PETR", 
            "raw_output": {
                "pts_bbox": {
                    "boxes_3d": [[0, 0, 0, 2, 3, 1, 0.1]],
                    "scores_3d": [0.88],
                    "labels_3d": [0]
                }
            },
            "metadata": {"inference_time": 0.18, "gpu_memory_used": 1800}
        }
    ]
    
    # å¤„ç†ç»“æœ
    for test in test_results:
        standardizer = create_standardizer(test["model"])
        result = standardizer.standardize(test["raw_output"], test["metadata"])
        comparator.add_result(result)
    
    # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Šå’Œå¯è§†åŒ–
    report = comparator.generate_comparison_report()
    print("\nğŸ“Š æ¯”è¾ƒæŠ¥å‘Š:")
    print(json.dumps(report, indent=2, ensure_ascii=False))
    
    comparator.create_visualizations()
    comparator.save_results()