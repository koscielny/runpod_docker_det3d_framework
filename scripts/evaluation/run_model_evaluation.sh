#!/bin/bash

# å¤šæ¨¡å‹è¯„æµ‹å’Œæ¯”è¾ƒè„šæœ¬
# é›†æˆå¥åº·æ£€æŸ¥ã€æ ‡å‡†åŒ–è¾“å‡ºã€æ¨¡å‹æ¯”è¾ƒåŠŸèƒ½
# é€‚ç”¨äºæ„å»ºä¸ªäººçš„å¤šæ¨¡å‹Dockerè¯„æµ‹é¡¹ç›®

set -e

# é¢œè‰²è¾“å‡º
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
OUTPUT_DIR="$SCRIPT_DIR/evaluation_results"
MODELS=("MapTR" "PETR" "StreamPETR" "TopoMLP" "VAD")

# æ—¥å¿—å‡½æ•°
log() {
    echo -e "${GREEN}[$(date '+%H:%M:%S')]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[$(date '+%H:%M:%S')] WARNING:${NC} $1"
}

error() {
    echo -e "${RED}[$(date '+%H:%M:%S')] ERROR:${NC} $1"
}

info() {
    echo -e "${BLUE}[$(date '+%H:%M:%S')] INFO:${NC} $1"
}

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    echo "ğŸš€ å¤šæ¨¡å‹è¯„æµ‹å’Œæ¯”è¾ƒå·¥å…·"
    echo "=========================="
    echo ""
    echo "ç”¨æ³•: $0 [é€‰é¡¹]"
    echo ""
    echo "æ¨¡å¼é€‰é¡¹:"
    echo "  --health-check         è¿è¡Œæ‰€æœ‰æ¨¡å‹çš„å¥åº·æ£€æŸ¥"
    echo "  --single-model MODEL   è¯„æµ‹å•ä¸ªæ¨¡å‹"
    echo "  --compare-models       æ¯”è¾ƒå¤šä¸ªæ¨¡å‹ (éœ€è¦å…ˆè¿è¡Œå•ä¸ªæ¨¡å‹è¯„æµ‹)"
    echo "  --full-evaluation      å®Œæ•´è¯„æµ‹æµç¨‹ (å¥åº·æ£€æŸ¥ + å•æ¨¡å‹è¯„æµ‹ + æ¯”è¾ƒ)"
    echo ""
    echo "é…ç½®é€‰é¡¹:"
    echo "  --data-path PATH       æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: /data/sample.txt)"
    echo "  --models LIST          æŒ‡å®šè¦è¯„æµ‹çš„æ¨¡å‹ï¼Œé€—å·åˆ†éš” (é»˜è®¤: æ‰€æœ‰æ¨¡å‹)"
    echo "  --output-dir DIR       è¾“å‡ºç›®å½• (é»˜è®¤: ./evaluation_results)"
    echo "  --skip-health          è·³è¿‡å¥åº·æ£€æŸ¥"
    echo "  --keep-containers      ä¿æŒå®¹å™¨è¿è¡Œä»¥ä¾¿è°ƒè¯•"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 --health-check                              # æ£€æŸ¥æ‰€æœ‰æ¨¡å‹å¥åº·çŠ¶æ€"
    echo "  $0 --single-model MapTR --data-path /data/test.txt  # è¯„æµ‹MapTRæ¨¡å‹"
    echo "  $0 --compare-models                            # æ¯”è¾ƒå·²è¯„æµ‹çš„æ¨¡å‹"
    echo "  $0 --full-evaluation --models MapTR,PETR      # å®Œæ•´è¯„æµ‹MapTRå’ŒPETR"
    echo ""
    echo "è¾“å‡ºæ–‡ä»¶:"
    echo "  - health_reports/       å¥åº·æ£€æŸ¥æŠ¥å‘Š"
    echo "  - model_outputs/        å•æ¨¡å‹è¾“å‡ºç»“æœ"
    echo "  - comparison/           æ¨¡å‹æ¯”è¾ƒåˆ†æ"
    echo "  - evaluation_summary.json  è¯„æµ‹æ€»ç»“"
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log "æ£€æŸ¥è¿è¡Œä¾èµ–..."
    
    # æ£€æŸ¥Docker
    if ! command -v docker &> /dev/null; then
        error "Docker æœªå®‰è£…æˆ–ä¸å¯ç”¨"
        exit 1
    fi
    
    # æ£€æŸ¥Python
    if ! command -v python3 &> /dev/null; then
        error "Python3 æœªå®‰è£…æˆ–ä¸å¯ç”¨"
        exit 1
    fi
    
    # æ£€æŸ¥å¿…è¦çš„PythonåŒ…
    python3 -c "import pandas, matplotlib, seaborn" 2>/dev/null || {
        warn "ç¼ºå°‘Pythonä¾èµ–åŒ…ï¼Œå°è¯•å®‰è£…..."
        pip install pandas matplotlib seaborn || {
            error "æ— æ³•å®‰è£…Pythonä¾èµ–åŒ…"
            exit 1
        }
    }
    
    info "ä¾èµ–æ£€æŸ¥å®Œæˆ"
}

# åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
setup_output_dirs() {
    log "åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„..."
    
    mkdir -p "$OUTPUT_DIR"/{health_reports,model_outputs,comparison,logs}
    
    # å¤åˆ¶æ ‡å‡†åŒ–å’Œæ¯”è¾ƒå·¥å…·
    cp "$SCRIPT_DIR/../tools/model_output_standard.py" "$OUTPUT_DIR/"
    cp "$SCRIPT_DIR/../tools/model_comparison.py" "$OUTPUT_DIR/"
    cp "$SCRIPT_DIR/../tools/health_check.py" "$OUTPUT_DIR/"
    
    info "è¾“å‡ºç›®å½•å·²åˆ›å»º: $OUTPUT_DIR"
}

# å¥åº·æ£€æŸ¥
run_health_checks() {
    local models_to_check=("$@")
    
    log "è¿è¡Œæ¨¡å‹å¥åº·æ£€æŸ¥..."
    
    for model in "${models_to_check[@]}"; do
        info "æ£€æŸ¥ $model å¥åº·çŠ¶æ€..."
        
        local image_name="${model,,}-model:latest"
        local health_output="$OUTPUT_DIR/health_reports/${model}_health.json"
        
        # æ£€æŸ¥Dockeré•œåƒæ˜¯å¦å­˜åœ¨
        if ! docker image inspect "$image_name" &> /dev/null; then
            warn "Dockeré•œåƒä¸å­˜åœ¨: $image_name"
            echo '{"error": "Docker image not found", "model": "'$model'", "image": "'$image_name'"}' > "$health_output"
            continue
        fi
        
        # è¿è¡Œå¥åº·æ£€æŸ¥
        if docker run --rm --gpus all -v "$OUTPUT_DIR:/output" "$image_name" \
            python3 /app/health_check.py --model "$model" --mode comprehensive > "$health_output" 2>&1; then
            info "âœ… $model å¥åº·æ£€æŸ¥å®Œæˆ"
        else
            warn "âŒ $model å¥åº·æ£€æŸ¥å¤±è´¥"
            echo '{"error": "Health check failed", "model": "'$model'"}' > "$health_output"
        fi
    done
    
    # ç”Ÿæˆå¥åº·æ£€æŸ¥æ±‡æ€»
    python3 -c "
import json
import os
from pathlib import Path

health_dir = Path('$OUTPUT_DIR/health_reports')
summary = {'timestamp': '$(date -Iseconds)', 'models': {}}

for health_file in health_dir.glob('*_health.json'):
    model_name = health_file.stem.replace('_health', '')
    try:
        with open(health_file) as f:
            data = json.load(f)
        
        if 'error' in data:
            summary['models'][model_name] = {'status': 'failed', 'error': data['error']}
        else:
            system_status = data.get('system_health', {}).get('status', 'unknown')
            model_status = data.get('model_health', {}).get('status', 'unknown')
            test_status = data.get('functionality_test', {}).get('status', 'unknown')
            
            overall_status = 'healthy'
            if any(s in ['error', 'broken', 'failed'] for s in [system_status, model_status, test_status]):
                overall_status = 'unhealthy'
            elif any(s in ['warning', 'partial', 'mostly_passed'] for s in [system_status, model_status, test_status]):
                overall_status = 'warning'
            
            summary['models'][model_name] = {
                'status': overall_status,
                'system': system_status,
                'model': model_status,
                'test': test_status
            }
    except Exception as e:
        summary['models'][model_name] = {'status': 'error', 'error': str(e)}

with open('$OUTPUT_DIR/health_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)

print('å¥åº·æ£€æŸ¥æ±‡æ€»:')
for model, status in summary['models'].items():
    status_icon = 'âœ…' if status['status'] == 'healthy' else 'âš ï¸' if status['status'] == 'warning' else 'âŒ'
    print(f'  {status_icon} {model}: {status[\"status\"]}')
"
}

# å•æ¨¡å‹è¯„æµ‹
run_single_model_evaluation() {
    local model="$1"
    local data_path="$2"
    local checkpoint_path="$3"
    
    info "è¯„æµ‹æ¨¡å‹: $model"
    
    if [ ! -f "$data_path" ]; then
        error "æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: $data_path"
        return 1
    fi
    
    local image_name="${model,,}-model:latest"
    local output_dir="$OUTPUT_DIR/model_outputs/$model"
    mkdir -p "$output_dir"
    
    # æ£€æŸ¥Dockeré•œåƒ
    if ! docker image inspect "$image_name" &> /dev/null; then
        error "Dockeré•œåƒä¸å­˜åœ¨: $image_nameï¼Œè¯·å…ˆæ„å»ºé•œåƒ"
        return 1
    fi
    
    # å‡†å¤‡è¾“å…¥æ•°æ®
    local input_dir="$output_dir/input"
    local output_results="$output_dir/output"
    mkdir -p "$input_dir" "$output_results"
    
    # å¤åˆ¶æ•°æ®æ–‡ä»¶
    cp "$data_path" "$input_dir/sample.txt"
    
    log "å¼€å§‹ $model æ¨ç†..."
    local start_time=$(date +%s.%N)
    
    # è¿è¡Œæ¨ç†ï¼Œè®°å½•GPUå†…å­˜ä½¿ç”¨
    if [ -n "$checkpoint_path" ] && [ -f "$checkpoint_path" ]; then
        # ä½¿ç”¨æŒ‡å®šçš„checkpoint
        docker run --rm --gpus all \
            -v "$checkpoint_path:/app/checkpoints/model.pth:ro" \
            -v "$input_dir:/app/input_data:ro" \
            -v "$output_results:/app/output_results:rw" \
            -v "$OUTPUT_DIR:/output:rw" \
            "$image_name" \
            python3 "/app/$model/inference.py" \
            --config "/app/$model/projects/configs/${model,,}/${model,,}_config.py" \
            --model-path "/app/checkpoints/model.pth" \
            --input "/app/input_data/sample.txt" \
            --output "/app/output_results/results.json" \
            --enable-monitoring
    else
        # ä½¿ç”¨é»˜è®¤é…ç½® (å‡è®¾æœ‰é¢„è®­ç»ƒæ¨¡å‹)
        warn "æœªæŒ‡å®šcheckpointï¼Œä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œæ¼”ç¤ºæ¨¡å¼"
        docker run --rm --gpus all \
            -v "$input_dir:/app/input_data:ro" \
            -v "$output_results:/app/output_results:rw" \
            -v "$OUTPUT_DIR:/output:rw" \
            "$image_name" \
            python3 "/app/$model/tools/demo.py" \
            --input "/app/input_data/sample.txt" \
            --output "/app/output_results/results.json"
    fi
    
    local end_time=$(date +%s.%N)
    local inference_time=$(echo "$end_time - $start_time" | bc -l)
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if [ -f "$output_results/results.json" ]; then
        log "âœ… $model æ¨ç†å®Œæˆï¼Œç”¨æ—¶: ${inference_time}s"
        
        # ä½¿ç”¨æ ‡å‡†åŒ–å·¥å…·å¤„ç†è¾“å‡º
        python3 -c "
import sys
sys.path.append('$OUTPUT_DIR')
from model_output_standard import create_standardizer
import json

# åŠ è½½åŸå§‹è¾“å‡º
with open('$output_results/results.json') as f:
    raw_output = json.load(f)

# åˆ›å»ºå…ƒæ•°æ®
metadata = {
    'model_version': 'v1.0',
    'config_file': 'default_config.py',
    'checkpoint_file': '${checkpoint_path:-default}',
    'inference_time': float('$inference_time'),
    'gpu_memory_used': 0.0  # TODO: ä»ç›‘æ§ä¸­è·å–
}

# æ ‡å‡†åŒ–è¾“å‡º
standardizer = create_standardizer('$model')
standardized = standardizer.standardize(raw_output, metadata)

# ä¿å­˜æ ‡å‡†åŒ–ç»“æœ
with open('$output_dir/standardized_output.json', 'w') as f:
    f.write(standardized.to_json())

print(f'$model æ ‡å‡†åŒ–è¾“å‡ºå·²ä¿å­˜')
"
        
    else
        error "âŒ $model æ¨ç†å¤±è´¥ï¼Œæœªç”Ÿæˆè¾“å‡ºæ–‡ä»¶"
        return 1
    fi
}

# æ¨¡å‹æ¯”è¾ƒ
run_model_comparison() {
    log "å¼€å§‹æ¨¡å‹æ¯”è¾ƒåˆ†æ..."
    
    local comparison_output="$OUTPUT_DIR/comparison"
    mkdir -p "$comparison_output"
    
    # ä½¿ç”¨æ¯”è¾ƒå·¥å…·
    python3 -c "
import sys
sys.path.append('$OUTPUT_DIR')
from model_comparison import ModelComparator
from model_output_standard import StandardOutput
import json
from pathlib import Path

# åˆ›å»ºæ¯”è¾ƒå™¨
comparator = ModelComparator('$comparison_output')

# åŠ è½½æ‰€æœ‰æ ‡å‡†åŒ–è¾“å‡º
output_dir = Path('$OUTPUT_DIR/model_outputs')
loaded_count = 0

for model_dir in output_dir.iterdir():
    if model_dir.is_dir():
        standardized_file = model_dir / 'standardized_output.json'
        if standardized_file.exists():
            try:
                with open(standardized_file) as f:
                    data = json.load(f)
                
                # é‡æ„StandardOutputå¯¹è±¡
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥å®Œæ•´é‡æ„å¯¹è±¡
                result = type('StandardOutput', (), data)()
                comparator.add_result(result)
                loaded_count += 1
                print(f'åŠ è½½æ¨¡å‹ç»“æœ: {model_dir.name}')
            except Exception as e:
                print(f'åŠ è½½ {model_dir.name} å¤±è´¥: {e}')

if loaded_count >= 2:
    # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
    report = comparator.generate_comparison_report()
    
    # åˆ›å»ºå¯è§†åŒ–
    try:
        comparator.create_visualizations()
    except Exception as e:
        print(f'åˆ›å»ºå¯è§†åŒ–æ—¶å‡ºé”™: {e}')
    
    # ä¿å­˜ç»“æœ
    comparator.save_results()
    
    print(f'âœ… æ¨¡å‹æ¯”è¾ƒå®Œæˆï¼Œå…±æ¯”è¾ƒ {loaded_count} ä¸ªæ¨¡å‹')
else:
    print(f'âŒ éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œå½“å‰åªæœ‰ {loaded_count} ä¸ª')
"
    
    if [ -f "$comparison_output/comparison_report.json" ]; then
        log "âœ… æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Šå·²ç”Ÿæˆ"
        
        # æ˜¾ç¤ºç®€è¦æ¯”è¾ƒç»“æœ
        python3 -c "
import json
with open('$comparison_output/comparison_report.json') as f:
    report = json.load(f)

print('\nğŸ“Š æ¨¡å‹æ¯”è¾ƒæ‘˜è¦:')
print(f'æ€»æ¨¡å‹æ•°: {report[\"summary\"][\"total_models\"]}')
print(f'æˆåŠŸæ¨¡å‹: {report[\"summary\"][\"successful_models\"]}')

if 'performance_ranking' in report:
    ranking = report['performance_ranking']
    print('\nğŸ† æ€§èƒ½æ’å:')
    for metric, model in ranking.items():
        print(f'  {metric}: {model}')

if 'insights' in report:
    print('\nğŸ’¡ åˆ†ææ´å¯Ÿ:')
    for insight in report['insights']:
        print(f'  â€¢ {insight}')
"
    else
        warn "æ¨¡å‹æ¯”è¾ƒæœªèƒ½ç”ŸæˆæŠ¥å‘Š"
    fi
}

# ç”Ÿæˆè¯„æµ‹æ€»ç»“
generate_evaluation_summary() {
    log "ç”Ÿæˆè¯„æµ‹æ€»ç»“..."
    
    python3 -c "
import json
import os
from pathlib import Path
from datetime import datetime

summary = {
    'evaluation_info': {
        'timestamp': datetime.now().isoformat(),
        'output_directory': '$OUTPUT_DIR',
        'script_version': '1.0'
    },
    'health_status': {},
    'model_evaluations': {},
    'comparison_results': {},
    'recommendations': []
}

# åŠ è½½å¥åº·æ£€æŸ¥ç»“æœ
health_file = Path('$OUTPUT_DIR/health_summary.json')
if health_file.exists():
    with open(health_file) as f:
        summary['health_status'] = json.load(f)

# åŠ è½½æ¨¡å‹è¯„æµ‹ç»“æœ
output_dir = Path('$OUTPUT_DIR/model_outputs')
for model_dir in output_dir.iterdir():
    if model_dir.is_dir() and (model_dir / 'standardized_output.json').exists():
        with open(model_dir / 'standardized_output.json') as f:
            data = json.load(f)
        
        summary['model_evaluations'][model_dir.name] = {
            'status': 'completed',
            'inference_time': data.get('metadata', {}).get('inference_time', 0),
            'detection_count': len(data.get('detections_3d', [])) if data.get('detections_3d') else 0,
            'map_element_count': len(data.get('map_elements', [])) if data.get('map_elements') else 0,
            'has_error': data.get('error') is not None
        }

# åŠ è½½æ¯”è¾ƒç»“æœ
comparison_file = Path('$OUTPUT_DIR/comparison/comparison_report.json')
if comparison_file.exists():
    with open(comparison_file) as f:
        summary['comparison_results'] = json.load(f)

# ç”Ÿæˆå»ºè®®
healthy_models = []
if 'models' in summary['health_status']:
    healthy_models = [model for model, status in summary['health_status']['models'].items() 
                     if status.get('status') == 'healthy']

if healthy_models:
    summary['recommendations'].append(f'æ¨èä½¿ç”¨çš„å¥åº·æ¨¡å‹: {", ".join(healthy_models)}')

if summary['comparison_results']:
    ranking = summary['comparison_results'].get('performance_ranking', {})
    if ranking:
        summary['recommendations'].append(f'æ¨ç†é€Ÿåº¦æœ€å¿«: {ranking.get("fastest_inference", "N/A")}')
        summary['recommendations'].append(f'å†…å­˜ä½¿ç”¨æœ€å°‘: {ranking.get("lowest_memory", "N/A")}')

# ä¿å­˜æ€»ç»“
with open('$OUTPUT_DIR/evaluation_summary.json', 'w') as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)

print('ğŸ“‹ è¯„æµ‹æ€»ç»“å·²ç”Ÿæˆ: $OUTPUT_DIR/evaluation_summary.json')
"
}

# ä¸»å‡½æ•°
main() {
    local mode=""
    local single_model=""
    local data_path="/data/sample.txt"
    local checkpoint_path=""
    local models_list=""
    local skip_health=false
    local keep_containers=false
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    while [[ $# -gt 0 ]]; do
        case $1 in
            --health-check)
                mode="health"
                shift
                ;;
            --single-model)
                mode="single"
                single_model="$2"
                shift 2
                ;;
            --compare-models)
                mode="compare"
                shift
                ;;
            --full-evaluation)
                mode="full"
                shift
                ;;
            --data-path)
                data_path="$2"
                shift 2
                ;;
            --checkpoint)
                checkpoint_path="$2"
                shift 2
                ;;
            --models)
                models_list="$2"
                shift 2
                ;;
            --output-dir)
                OUTPUT_DIR="$2"
                shift 2
                ;;
            --skip-health)
                skip_health=true
                shift
                ;;
            --keep-containers)
                keep_containers=true
                shift
                ;;
            --help)
                show_help
                exit 0
                ;;
            *)
                error "æœªçŸ¥é€‰é¡¹: $1"
                show_help
                exit 1
                ;;
        esac
    done
    
    # è®¾ç½®è¦è¯„æµ‹çš„æ¨¡å‹åˆ—è¡¨
    local models_to_eval=()
    if [ -n "$models_list" ]; then
        IFS=',' read -ra models_to_eval <<< "$models_list"
    else
        models_to_eval=("${MODELS[@]}")
    fi
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å¼ï¼Œæ˜¾ç¤ºå¸®åŠ©
    if [ -z "$mode" ]; then
        show_help
        exit 1
    fi
    
    log "ğŸš€ å¼€å§‹å¤šæ¨¡å‹è¯„æµ‹æµç¨‹"
    info "è¾“å‡ºç›®å½•: $OUTPUT_DIR"
    info "è¯„æµ‹æ¨¡å‹: ${models_to_eval[*]}"
    
    # æ£€æŸ¥ä¾èµ–å’Œè®¾ç½®
    check_dependencies
    setup_output_dirs
    
    # æ‰§è¡Œç›¸åº”çš„æ¨¡å¼
    case $mode in
        "health")
            run_health_checks "${models_to_eval[@]}"
            ;;
        "single")
            if [ -z "$single_model" ]; then
                error "å•æ¨¡å‹æ¨¡å¼éœ€è¦æŒ‡å®šæ¨¡å‹åç§°"
                exit 1
            fi
            
            if [ "$skip_health" = false ]; then
                run_health_checks "$single_model"
            fi
            
            run_single_model_evaluation "$single_model" "$data_path" "$checkpoint_path"
            ;;
        "compare")
            run_model_comparison
            ;;
        "full")
            if [ "$skip_health" = false ]; then
                run_health_checks "${models_to_eval[@]}"
            fi
            
            # è¯„æµ‹æ¯ä¸ªæ¨¡å‹
            for model in "${models_to_eval[@]}"; do
                run_single_model_evaluation "$model" "$data_path" "$checkpoint_path"
            done
            
            # æ¯”è¾ƒæ¨¡å‹
            run_model_comparison
            
            # ç”Ÿæˆæ€»ç»“
            generate_evaluation_summary
            ;;
    esac
    
    log "ğŸ‰ è¯„æµ‹æµç¨‹å®Œæˆï¼"
    info "ç»“æœä¿å­˜åœ¨: $OUTPUT_DIR"
    
    # æ˜¾ç¤ºç»“æœæ–‡ä»¶
    echo ""
    echo "ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:"
    find "$OUTPUT_DIR" -name "*.json" -o -name "*.png" -o -name "*.csv" | head -10 | while read file; do
        echo "  $(basename "$file")"
    done
    
    if [ "$(find "$OUTPUT_DIR" -name "*.json" -o -name "*.png" -o -name "*.csv" | wc -l)" -gt 10 ]; then
        echo "  ... å’Œæ›´å¤šæ–‡ä»¶"
    fi
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"