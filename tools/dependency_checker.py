#!/usr/bin/env python3
"""
Dockerå®¹å™¨ä¾èµ–æ£€æŸ¥å·¥å…·
ç”¨äºé¦–æ¬¡è¿è¡Œæ—¶éªŒè¯æ‰€æœ‰ä¾èµ–æ˜¯å¦å¥å…¨ï¼Œç¡®ä¿æ¨¡å‹å¯ä»¥æ­£å¸¸è¿è¡Œ
"""

import os
import sys
import json
import time
import subprocess
import importlib
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import warnings
import mmdet
import mmdet3d
import mmcv
# æŠ‘åˆ¶è­¦å‘Šä¿¡æ¯ï¼Œé¿å…å½±å“æ£€æŸ¥ç»“æœæ˜¾ç¤º
warnings.filterwarnings("ignore")

class DependencyChecker:
    """ä¾èµ–æ£€æŸ¥å™¨"""
    
    def __init__(self, model_name: str = "Unknown"):
        # ä¿æŒæ­£ç¡®çš„æ¨¡å‹åç§°æ ¼å¼
        self.model_name = model_name
        self.display_name = model_name.upper()
        self.checks_passed = 0
        self.checks_total = 0
        self.errors = []
        self.warnings = []
        self.start_time = time.time()
    
    def log_result(self, check_name: str, status: bool, details: str = "", is_critical: bool = True):
        """è®°å½•æ£€æŸ¥ç»“æœ"""
        self.checks_total += 1
        if status:
            self.checks_passed += 1
            print(f"âœ… {check_name}: {details}")
        else:
            emoji = "âŒ" if is_critical else "âš ï¸"
            print(f"{emoji} {check_name}: {details}")
            if is_critical:
                self.errors.append(f"{check_name}: {details}")
            else:
                self.warnings.append(f"{check_name}: {details}")
    
    def check_python_environment(self) -> bool:
        """æ£€æŸ¥Pythonç¯å¢ƒ"""
        print("\nğŸ æ£€æŸ¥Pythonç¯å¢ƒ...")
        
        # Pythonç‰ˆæœ¬
        python_version = sys.version_info
        required_major, required_minor = 3, 7
        version_ok = python_version.major >= required_major and python_version.minor >= required_minor
        self.log_result(
            "Pythonç‰ˆæœ¬", 
            version_ok,
            f"{python_version.major}.{python_version.minor}.{python_version.micro} (éœ€è¦ >= {required_major}.{required_minor})"
        )
        
        # Pythonè·¯å¾„
        python_path = sys.executable
        self.log_result(
            "Pythonè·¯å¾„", 
            os.path.exists(python_path),
            python_path
        )
        
        # æ£€æŸ¥pip
        try:
            import pip
            pip_version = pip.__version__
            self.log_result("pip", True, f"ç‰ˆæœ¬ {pip_version}")
        except ImportError:
            self.log_result("pip", False, "pipæœªå®‰è£…")
        
        return version_ok
    
    def check_core_dependencies(self) -> bool:
        """æ£€æŸ¥æ ¸å¿ƒPythonä¾èµ–"""
        print("\nğŸ“¦ æ£€æŸ¥æ ¸å¿ƒä¾èµ–...")
        
        # æ ¸å¿ƒä¾èµ–åˆ—è¡¨
        core_deps = {
            'torch': 'PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶',
            'torchvision': 'PyTorchè§†è§‰åº“',
            'numpy': 'æ•°å€¼è®¡ç®—åº“',
            'opencv-python': 'è®¡ç®—æœºè§†è§‰åº“',  # å®é™…æ¨¡å—åæ˜¯cv2
            'pillow': 'å›¾åƒå¤„ç†åº“',  # å®é™…æ¨¡å—åæ˜¯PIL
            'matplotlib': 'ç»˜å›¾åº“',
            'psutil': 'ç³»ç»Ÿç›‘æ§åº“',
        }
        
        # ç‰¹æ®Šæ¨¡å—åæ˜ å°„
        module_mapping = {
            'opencv-python': 'cv2',
            'pillow': 'PIL'
        }
        
        all_ok = True
        for pkg_name, description in core_deps.items():
            module_name = module_mapping.get(pkg_name, pkg_name)
            try:
                module = importlib.import_module(module_name)
                version = getattr(module, '__version__', 'Unknown')
                self.log_result(f"{pkg_name}", True, f"{description} - ç‰ˆæœ¬ {version}")
            except ImportError as e:
                self.log_result(f"{pkg_name}", False, f"{description} - æœªå®‰è£…: {e}")
                all_ok = False
        
        return all_ok
    def check_ai_frameworks(self) -> bool:
        """æ£€æŸ¥AIæ¡†æ¶å’Œåº“ - ç®€åŒ–ç‰ˆæœ¬"""
        print("\nğŸ¤– æ£€æŸ¥AIæ¡†æ¶...")
        
        ai_deps = {
            'mmcv': 'OpenMMLabè®¡ç®—æœºè§†è§‰åº“',
            'mmdet': 'MMDetectionç›®æ ‡æ£€æµ‹åº“',
            'mmdet3d': 'MMDetection3D 3Dæ£€æµ‹åº“',
            'scipy': 'ç§‘å­¦è®¡ç®—åº“',
            'scikit-image': 'å›¾åƒå¤„ç†åº“',
            'shapely': 'å‡ ä½•è®¡ç®—åº“',
            'timm': 'PyTorchå›¾åƒæ¨¡å‹åº“',
        }
        
        # Module name mapping for special cases
        module_mapping = {
            'scikit-image': 'skimage',
            'scikit-learn': 'sklearn'
        }
        
        essential_count = 0
        available_count = 0
        
        for pkg_name, description in ai_deps.items():
            module_name = module_mapping.get(pkg_name, pkg_name)
            
            # Use simplified import strategy for MM modules and others
            if pkg_name in ['mmcv', 'mmdet', 'mmdet3d']:
                success, version = self._simple_import_mm_module(module_name)
            else:
                success, version = self._simple_import_module(module_name)
            
            if success:
                status_msg = f"{description} - ç‰ˆæœ¬ {version}"
                self.log_result(f"{pkg_name}", True, status_msg, is_critical=False)
                available_count += 1
                if pkg_name in ['mmcv', 'scipy']:  # Core frameworks
                    essential_count += 1
            else:
                is_critical = pkg_name in ['mmcv', 'scipy']
                self.log_result(f"{pkg_name}", False, f"{description} - æœªå®‰è£…", is_critical=is_critical)
        
        return essential_count >= 1  # At least one core framework available

    def check_gpu_support(self) -> bool:
        """æ£€æŸ¥GPUæ”¯æŒ"""
        print("\nğŸ® æ£€æŸ¥GPUæ”¯æŒ...")
        
        # æ£€æŸ¥CUDAå¯ç”¨æ€§
        try:
            import torch
            cuda_available = torch.cuda.is_available()
            if cuda_available:
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                cuda_version = torch.version.cuda
                self.log_result(
                    "CUDAå¯ç”¨æ€§", True, 
                    f"âœ“ {gpu_count}ä¸ªGPU - {gpu_name} (CUDA {cuda_version})"
                )
                
                # æ£€æŸ¥GPUå†…å­˜
                if gpu_count > 0:
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    self.log_result(
                        "GPUå†…å­˜", True,
                        f"{gpu_memory:.1f}GB"
                    )
            else:
                self.log_result("CUDAå¯ç”¨æ€§", False, "CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU", is_critical=False)
                
        except ImportError:
            self.log_result("PyTorch", False, "PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥GPU")
            return False
        
        # æ£€æŸ¥nvidia-smi
        try:
            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                # è§£æGPUä¿¡æ¯
                lines = result.stdout.split('\n')
                driver_line = [line for line in lines if 'Driver Version' in line]
                if driver_line:
                    driver_info = driver_line[0].split()
                    driver_version = next((item for item in driver_info if '.' in item and item.replace('.', '').isdigit()), "Unknown")
                    self.log_result("NVIDIAé©±åŠ¨", True, f"ç‰ˆæœ¬ {driver_version}")
                else:
                    self.log_result("NVIDIAé©±åŠ¨", True, "å·²å®‰è£…")
            else:
                self.log_result("NVIDIAé©±åŠ¨", False, "nvidia-smiå‘½ä»¤å¤±è´¥", is_critical=False)
        except (subprocess.TimeoutExpired, FileNotFoundError):
            self.log_result("NVIDIAé©±åŠ¨", False, "nvidia-smiä¸å¯ç”¨", is_critical=False)
        
        return cuda_available if 'torch' in locals() else False
    
    def check_model_files(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’Œé…ç½®"""
        print("\nğŸ“ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
        
        # æ£€æŸ¥å…³é”®ç›®å½•
        important_paths = {
            '/app': 'åº”ç”¨æ ¹ç›®å½•',
            f'/app/{self.model_name}': f'{self.display_name}æ¨¡å‹ç›®å½•',
            '/app/tools': 'å·¥å…·ç›®å½•',
            '/app/tools/health_check.py': 'å¥åº·æ£€æŸ¥è„šæœ¬',
            '/app/tools/model_output_standard.py': 'æ ‡å‡†è¾“å‡ºè„šæœ¬',
        }
        
        all_ok = True
        for path, description in important_paths.items():
            exists = os.path.exists(path)
            if not exists and '/app/tools' in path:
                # toolsç›®å½•ä¸æ˜¯å¿…éœ€çš„
                self.log_result(f"{description}", exists, path, is_critical=False)
            else:
                self.log_result(f"{description}", exists, path)
                if not exists and path != f'/app/{self.model_name}':  # æ¨¡å‹ç›®å½•å¯èƒ½ä¸å­˜åœ¨
                    all_ok = False
        
        # æ£€æŸ¥æ¨¡å‹ç‰¹å®šæ–‡ä»¶
        model_dir = f'/app/{self.model_name}'
        if os.path.exists(model_dir):
            model_files = {
                f'{model_dir}/inference.py': 'æ¨ç†è„šæœ¬',
                f'{model_dir}/requirements.txt': 'ä¾èµ–æ–‡ä»¶',
            }
            
            for file_path, description in model_files.items():
                exists = os.path.exists(file_path)
                self.log_result(f"{description}", exists, file_path, is_critical=(description == 'æ¨ç†è„šæœ¬'))
        
        return all_ok
    
    def check_system_resources(self) -> bool:
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        print("\nğŸ’¾ æ£€æŸ¥ç³»ç»Ÿèµ„æº...")
        
        try:
            import psutil
            
            # å†…å­˜æ£€æŸ¥
            memory = psutil.virtual_memory()
            memory_gb = memory.total / (1024**3)
            memory_available_gb = memory.available / (1024**3)
            memory_ok = memory_gb >= 0.5  # è‡³å°‘500MB
            
            self.log_result(
                "ç³»ç»Ÿå†…å­˜", memory_ok,
                f"{memory_gb:.1f}GB æ€»é‡, {memory_available_gb:.1f}GB å¯ç”¨ ({memory.percent:.1f}% ä½¿ç”¨)"
            )
            
            # ç£ç›˜ç©ºé—´æ£€æŸ¥
            disk = psutil.disk_usage('/')
            disk_free_gb = disk.free / (1024**3)
            disk_ok = disk_free_gb >= 1.0  # è‡³å°‘1GBç©ºé—²
            
            self.log_result(
                "ç£ç›˜ç©ºé—´", disk_ok,
                f"{disk_free_gb:.1f}GB å¯ç”¨ç©ºé—´"
            )
            
            # CPUæ£€æŸ¥
            cpu_count = psutil.cpu_count()
            self.log_result(
                "CPUæ ¸å¿ƒ", True,
                f"{cpu_count}ä¸ªCPUæ ¸å¿ƒ"
            )
            
            return memory_ok and disk_ok
            
        except ImportError:
            self.log_result("psutil", False, "ç³»ç»Ÿç›‘æ§åº“æœªå®‰è£…")
            return False
    
    def check_network_connectivity(self) -> bool:
        """æ£€æŸ¥ç½‘ç»œè¿æ¥"""
        print("\nğŸŒ æ£€æŸ¥ç½‘ç»œè¿æ¥...")
        
        # æµ‹è¯•å¸¸ç”¨çš„æœºå™¨å­¦ä¹ èµ„æº
        test_urls = [
            ('huggingface.co', 'Hugging Face'),
            ('download.pytorch.org', 'PyTorchä¸‹è½½'),
            ('pypi.org', 'PythonåŒ…ç´¢å¼•'),
        ]
        
        success_count = 0
        for host, description in test_urls:
            try:
                result = subprocess.run(
                    ['ping', '-c', '1', '-W', '3', host], 
                    capture_output=True, 
                    timeout=5
                )
                success = result.returncode == 0
                if success:
                    success_count += 1
                self.log_result(
                    f"ç½‘ç»œè¿æ¥-{description}", success, 
                    host, is_critical=False
                )
            except (subprocess.TimeoutExpired, FileNotFoundError):
                self.log_result(
                    f"ç½‘ç»œè¿æ¥-{description}", False,
                    f"æ— æ³•ping {host}", is_critical=False
                )
        
        return success_count > 0  # è‡³å°‘ä¸€ä¸ªè¿æ¥æˆåŠŸ
    
    def run_simple_model_test(self) -> bool:
        """è¿è¡Œç®€å•çš„æ¨¡å‹æµ‹è¯•"""
        print("\nğŸ§ª è¿è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•...")
        
        try:
            # æµ‹è¯•PyTorchåŸºç¡€åŠŸèƒ½
            import torch
            
            # åˆ›å»ºç®€å•å¼ é‡æµ‹è¯•
            x = torch.randn(2, 3)
            y = torch.randn(3, 2)
            result = torch.mm(x, y)
            
            tensor_test = result.shape == (2, 2)
            self.log_result("PyTorchå¼ é‡è¿ç®—", tensor_test, "åŸºç¡€å¼ é‡è¿ç®—æ­£å¸¸")
            
            # æµ‹è¯•GPUåŠŸèƒ½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                try:
                    x_gpu = x.cuda()
                    gpu_test = x_gpu.device.type == 'cuda'
                    self.log_result("GPUå¼ é‡è¿ç®—", gpu_test, "GPUå¼ é‡åˆ›å»ºæ­£å¸¸")
                except Exception as e:
                    self.log_result("GPUå¼ é‡è¿ç®—", False, f"GPUæµ‹è¯•å¤±è´¥: {e}")
                    gpu_test = False
            else:
                gpu_test = True  # å¦‚æœæ²¡æœ‰GPUï¼Œè·³è¿‡æµ‹è¯•
                self.log_result("GPUå¼ é‡è¿ç®—", True, "è·³è¿‡ (æ— GPU)", is_critical=False)
            
            # æµ‹è¯•å†…å­˜æ¸…ç†
            del x, y, result
            if 'x_gpu' in locals():
                del x_gpu
            
            import gc
            collected = gc.collect()
            self.log_result("å†…å­˜æ¸…ç†", True, f"å›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
            
            return tensor_test
            
        except ImportError:
            self.log_result("PyTorchæµ‹è¯•", False, "PyTorchä¸å¯ç”¨")
            return False
        except Exception as e:
            self.log_result("PyTorchæµ‹è¯•", False, f"æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        elapsed_time = time.time() - self.start_time
        
        report = {
            'model_name': self.model_name,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'elapsed_time': f"{elapsed_time:.2f}ç§’",
            'checks_passed': self.checks_passed,
            'checks_total': self.checks_total,
            'success_rate': f"{(self.checks_passed/self.checks_total*100):.1f}%" if self.checks_total > 0 else "0%",
            'errors': self.errors,
            'warnings': self.warnings,
            'status': 'READY' if len(self.errors) == 0 else 'ISSUES_FOUND'
        }
        
        return report
    
    def run_full_check(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„ä¾èµ–æ£€æŸ¥"""
        print("=" * 70)
        print(f"ğŸ” Dockerå®¹å™¨ä¾èµ–æ£€æŸ¥ - {self.display_name}æ¨¡å‹")
        print("=" * 70)
        
        # æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
        checks = [
            self.check_python_environment,
            self.check_core_dependencies,
            self.check_ai_frameworks,
            self.check_gpu_support,
            self.check_model_files,
            self.check_system_resources,
            self.check_network_connectivity,
            self.run_simple_model_test,
        ]
        
        for check in checks:
            try:
                check()
            except Exception as e:
                print(f"âŒ æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}")
                self.errors.append(f"æ£€æŸ¥è¿‡ç¨‹é”™è¯¯: {e}")
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report()
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 70)
        print("ğŸ“Š æ£€æŸ¥ç»“æœæ±‡æ€»")
        print("=" * 70)
        
        print(f"ğŸ·ï¸  æ¨¡å‹: {report['model_name']}")
        print(f"â±ï¸  ç”¨æ—¶: {report['elapsed_time']}")
        print(f"âœ… é€šè¿‡: {report['checks_passed']}/{report['checks_total']} ({report['success_rate']})")
        
        if report['errors']:
            print(f"\nâŒ ä¸¥é‡é—®é¢˜ ({len(report['errors'])}ä¸ª):")
            for error in report['errors']:
                print(f"   â€¢ {error}")
        
        if report['warnings']:
            print(f"\nâš ï¸  è­¦å‘Š ({len(report['warnings'])}ä¸ª):")
            for warning in report['warnings']:
                print(f"   â€¢ {warning}")
        
        # æœ€ç»ˆçŠ¶æ€
        if report['status'] == 'READY':
            print(f"\nğŸ‰ çŠ¶æ€: å®¹å™¨ä¾èµ–å¥å…¨ï¼Œ{self.display_name}æ¨¡å‹å¯ä»¥è¿è¡Œï¼")
            print(f"ğŸ“ å»ºè®®: å¯ä»¥å¼€å§‹ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†")
        else:
            print(f"\nâš ï¸  çŠ¶æ€: å‘ç°é—®é¢˜ï¼Œéœ€è¦ä¿®å¤åæ‰èƒ½è¿è¡Œæ¨¡å‹")
            print(f"ğŸ“ å»ºè®®: è¯·è§£å†³ä¸Šè¿°é”™è¯¯åé‡æ–°æ£€æŸ¥")
        
        # å¿«é€Ÿè§£å†³æ–¹æ¡ˆ
        if report['errors'] or report['warnings']:
            print(f"\nğŸ› ï¸  å¿«é€Ÿè§£å†³æ–¹æ¡ˆ:")
            if any('æœªå®‰è£…' in error for error in report['errors']):
                print(f"   â€¢ å®‰è£…ç¼ºå¤±ä¾èµ–: pip install torch torchvision numpy opencv-python")
            if any('å†…å­˜' in error for error in report['errors']):
                print(f"   â€¢ å†…å­˜ä¼˜åŒ–: python /app/tools/memory_optimizer.py --cleanup")
            if any('GPU' in error for error in report['errors']):
                print(f"   â€¢ æ£€æŸ¥GPU: nvidia-smi")
        
        print("=" * 70)
        
        return report['status'] == 'READY'

    # ...existing code...

    def _simple_import_mm_module(self, module_name: str) -> Tuple[bool, str]:
        """
        Simple import method for MM modules (mmcv, mmdet, mmdet3d)
        Using the verified approach that works for these modules
        
        Args:
            module_name: Name of the module to import
            
        Returns:
            Tuple of (success, version)
        """
        try:
            module = importlib.import_module(module_name)
            version = getattr(module, '__version__', 'Unknown')
            return True, version
        except ImportError:
            return False, 'Unknown'
        except Exception:
            return False, 'Unknown'

    def _simple_import_module(self, module_name: str) -> Tuple[bool, str]:
        """
        Simple import method for other modules
        
        Args:
            module_name: Name of the module to import
            
        Returns:
            Tuple of (success, version)
        """
        try:
            module = importlib.import_module(module_name)
            # Try common version attributes
            version_attrs = ['__version__', 'VERSION', 'version']
            for attr in version_attrs:
                if hasattr(module, attr):
                    version = getattr(module, attr)
                    if version:
                        return True, str(version)
            return True, 'Unknown'
        except ImportError:
            return False, 'Unknown'
        except Exception:
            return False, 'Unknown'

def detect_model_name() -> str:
    """è‡ªåŠ¨æ£€æµ‹å½“å‰æ¨¡å‹åç§°"""
    cwd = os.getcwd()
    
    # ä»å½“å‰ç›®å½•æ¨æ–­æ¨¡å‹åç§°
    model_names = ['MapTR', 'PETR', 'StreamPETR', 'TopoMLP', 'VAD']
    for model in model_names:
        if model.lower() in cwd.lower() or os.path.exists(f'/app/{model}'):
            return model
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    model_env = os.environ.get('MODEL_NAME', '').upper()
    if model_env in model_names:
        return model_env
    
    return "Unknown"

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Dockerå®¹å™¨ä¾èµ–æ£€æŸ¥å·¥å…·')
    parser.add_argument('--model', type=str, help='æŒ‡å®šæ¨¡å‹åç§° (MapTR/PETR/StreamPETR/TopoMLP/VAD)')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæ£€æŸ¥æ¨¡å¼ï¼ˆè·³è¿‡ç½‘ç»œæµ‹è¯•ï¼‰')
    parser.add_argument('--json', action='store_true', help='ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ')
    
    args = parser.parse_args()
    
    # ç¡®å®šæ¨¡å‹åç§°
    model_name = args.model if args.model else detect_model_name()
    
    # åˆ›å»ºæ£€æŸ¥å™¨
    checker = DependencyChecker(model_name)
    
    # æ‰§è¡Œæ£€æŸ¥
    if args.quick:
        # å¿«é€Ÿæ¨¡å¼ï¼Œè·³è¿‡ç½‘ç»œæ£€æŸ¥
        original_method = checker.check_network_connectivity
        checker.check_network_connectivity = lambda: True
        
    success = checker.run_full_check()
    
    # è¾“å‡ºç»“æœ
    if args.json:
        report = checker.generate_report()
        print(json.dumps(report, ensure_ascii=False, indent=2))
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()