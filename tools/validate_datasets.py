#!/usr/bin/env python3
"""
æ•°æ®é›†éªŒè¯è„šæœ¬
éªŒè¯å·²ä¸‹è½½çš„ nuScenes, Waymo, Argoverse æ•°æ®é›†çš„å®Œæ•´æ€§å’Œå¯ç”¨æ€§
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import subprocess

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dataset_validation.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class DatasetValidator:
    """æ•°æ®é›†éªŒè¯å™¨åŸºç±»"""
    
    def __init__(self, dataset_root: str):
        self.dataset_root = Path(dataset_root)
        self.validation_results = {}
    
    def validate(self) -> Dict:
        """éªŒè¯æ•°æ®é›†"""
        raise NotImplementedError
    
    def check_file_exists(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return Path(file_path).exists()
    
    def check_directory_exists(self, dir_path: str) -> bool:
        """æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨"""
        return Path(dir_path).is_dir()
    
    def get_file_size(self, file_path: str) -> int:
        """è·å–æ–‡ä»¶å¤§å° (bytes)"""
        try:
            return Path(file_path).stat().st_size
        except OSError:
            return 0
    
    def get_directory_size(self, dir_path: str) -> int:
        """è·å–ç›®å½•å¤§å° (bytes)"""
        try:
            total_size = 0
            for path in Path(dir_path).rglob('*'):
                if path.is_file():
                    total_size += path.stat().st_size
            return total_size
        except OSError:
            return 0

class NuScenesValidator(DatasetValidator):
    """nuScenes æ•°æ®é›†éªŒè¯å™¨"""
    
    def validate(self) -> Dict:
        logger.info("éªŒè¯ nuScenes æ•°æ®é›†...")
        
        results = {
            'dataset': 'nuScenes',
            'root_path': str(self.dataset_root),
            'status': 'unknown',
            'details': {}
        }
        
        try:
            # æ£€æŸ¥ v1.0-mini ç›®å½•
            mini_path = self.dataset_root / 'v1.0-mini'
            if not self.check_directory_exists(mini_path):
                results['status'] = 'failed'
                results['details']['error'] = 'v1.0-mini directory not found'
                return results
            
            # éªŒè¯å…³é”®æ–‡ä»¶
            required_files = [
                'scene.json', 'sample.json', 'sample_data.json', 
                'ego_pose.json', 'calibrated_sensor.json', 'sensor.json'
            ]
            
            missing_files = []
            for file_name in required_files:
                file_path = mini_path / file_name
                if not self.check_file_exists(file_path):
                    missing_files.append(file_name)
            
            if missing_files:
                results['status'] = 'incomplete'
                results['details']['missing_files'] = missing_files
            else:
                results['status'] = 'valid'
            
            # éªŒè¯æ•°æ®é›†å†…å®¹
            try:
                import json
                
                # è¯»å–åœºæ™¯ä¿¡æ¯
                scene_file = mini_path / 'scene.json'
                with open(scene_file, 'r') as f:
                    scenes = json.load(f)
                
                # è¯»å–æ ·æœ¬ä¿¡æ¯
                sample_file = mini_path / 'sample.json'
                with open(sample_file, 'r') as f:
                    samples = json.load(f)
                
                results['details'].update({
                    'scene_count': len(scenes),
                    'sample_count': len(samples),
                    'dataset_size_mb': round(self.get_directory_size(self.dataset_root) / (1024 * 1024), 2)
                })
                
                # éªŒè¯ API å¯ç”¨æ€§
                try:
                    from nuscenes.nuscenes import NuScenes
                    nusc = NuScenes(version='v1.0-mini', dataroot=str(self.dataset_root), verbose=False)
                    results['details']['api_available'] = True
                    results['details']['api_scene_count'] = len(nusc.scene)
                    results['details']['api_sample_count'] = len(nusc.sample)
                    
                    # æµ‹è¯•åŠ è½½ç¬¬ä¸€ä¸ªåœºæ™¯
                    if len(nusc.scene) > 0:
                        first_scene = nusc.scene[0]
                        results['details']['first_scene_token'] = first_scene['token']
                        results['details']['first_scene_name'] = first_scene['name']
                        
                except ImportError:
                    results['details']['api_available'] = False
                    results['details']['api_error'] = 'nuscenes-devkit not installed'
                except Exception as e:
                    results['details']['api_available'] = False
                    results['details']['api_error'] = str(e)
                
            except Exception as e:
                results['details']['metadata_error'] = str(e)
                
        except Exception as e:
            results['status'] = 'error'
            results['details']['error'] = str(e)
        
        return results

class WaymoValidator(DatasetValidator):
    """Waymo Open Dataset éªŒè¯å™¨"""
    
    def validate(self) -> Dict:
        logger.info("éªŒè¯ Waymo Open Dataset...")
        
        results = {
            'dataset': 'Waymo',
            'root_path': str(self.dataset_root),
            'status': 'unknown',
            'details': {}
        }
        
        try:
            # æ£€æŸ¥éªŒè¯é›†ç›®å½•
            validation_path = self.dataset_root / 'validation'
            if not self.check_directory_exists(validation_path):
                results['status'] = 'failed'
                results['details']['error'] = 'validation directory not found'
                return results
            
            # æŸ¥æ‰¾ TFRecord æ–‡ä»¶
            tfrecord_files = list(validation_path.glob('*.tfrecord'))
            
            if not tfrecord_files:
                results['status'] = 'failed'
                results['details']['error'] = 'no tfrecord files found'
                return results
            
            results['status'] = 'valid'
            results['details'].update({
                'tfrecord_count': len(tfrecord_files),
                'dataset_size_mb': round(self.get_directory_size(validation_path) / (1024 * 1024), 2)
            })
            
            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
            file_sizes = []
            for tf_file in tfrecord_files[:5]:  # æ£€æŸ¥å‰5ä¸ªæ–‡ä»¶
                size_mb = round(self.get_file_size(tf_file) / (1024 * 1024), 2)
                file_sizes.append({
                    'filename': tf_file.name,
                    'size_mb': size_mb
                })
            
            results['details']['sample_files'] = file_sizes
            
            # éªŒè¯ API å¯ç”¨æ€§
            try:
                import tensorflow as tf
                results['details']['tensorflow_available'] = True
                results['details']['tensorflow_version'] = tf.__version__
                
                try:
                    from waymo_open_dataset import dataset_pb2
                    results['details']['waymo_api_available'] = True
                    
                    # å°è¯•è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ç¬¬ä¸€ä¸ªè®°å½•
                    if tfrecord_files:
                        first_file = str(tfrecord_files[0])
                        dataset = tf.data.TFRecordDataset([first_file])
                        
                        for i, data in enumerate(dataset.take(1)):
                            frame = dataset_pb2.Frame()
                            frame.ParseFromString(data.numpy())
                            
                            results['details']['sample_frame'] = {
                                'timestamp_micros': frame.timestamp_micros,
                                'context_name': frame.context.name,
                                'camera_labels_count': len(frame.camera_labels),
                                'laser_labels_count': len(frame.laser_labels)
                            }
                            break
                        
                except ImportError as e:
                    results['details']['waymo_api_available'] = False
                    results['details']['waymo_api_error'] = f'waymo-open-dataset not installed: {e}'
                except Exception as e:
                    results['details']['waymo_api_available'] = False
                    results['details']['waymo_api_error'] = str(e)
                    
            except ImportError:
                results['details']['tensorflow_available'] = False
                results['details']['api_error'] = 'tensorflow not installed'
                
        except Exception as e:
            results['status'] = 'error'
            results['details']['error'] = str(e)
        
        return results

class ArgoverseValidator(DatasetValidator):
    """Argoverse 2 æ•°æ®é›†éªŒè¯å™¨"""
    
    def validate(self) -> Dict:
        logger.info("éªŒè¯ Argoverse 2 æ•°æ®é›†...")
        
        results = {
            'dataset': 'Argoverse2',
            'root_path': str(self.dataset_root),
            'status': 'unknown',
            'details': {}
        }
        
        try:
            # æ£€æŸ¥ motion_forecasting ç›®å½•
            motion_path = self.dataset_root / 'motion_forecasting'
            if not self.check_directory_exists(motion_path):
                results['status'] = 'failed'
                results['details']['error'] = 'motion_forecasting directory not found'
                return results
            
            # æŸ¥æ‰¾éªŒè¯é›†æ–‡ä»¶
            val_path = motion_path / 'val'
            parquet_files = []
            
            if self.check_directory_exists(val_path):
                parquet_files = list(val_path.glob('*.parquet'))
            else:
                # æ£€æŸ¥æ ¹ç›®å½•ä¸‹çš„ parquet æ–‡ä»¶
                parquet_files = list(motion_path.glob('*.parquet'))
            
            if not parquet_files:
                results['status'] = 'failed'
                results['details']['error'] = 'no parquet files found'
                return results
            
            results['status'] = 'valid'
            results['details'].update({
                'scenario_count': len(parquet_files),
                'dataset_size_mb': round(self.get_directory_size(motion_path) / (1024 * 1024), 2)
            })
            
            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
            file_sizes = []
            for pq_file in parquet_files[:5]:  # æ£€æŸ¥å‰5ä¸ªæ–‡ä»¶
                size_kb = round(self.get_file_size(pq_file) / 1024, 2)
                file_sizes.append({
                    'filename': pq_file.name,
                    'size_kb': size_kb
                })
            
            results['details']['sample_files'] = file_sizes
            
            # éªŒè¯ API å¯ç”¨æ€§
            try:
                from av2.datasets.motion_forecasting import scenario_serialization
                results['details']['av2_api_available'] = True
                
                # å°è¯•åŠ è½½ç¬¬ä¸€ä¸ªåœºæ™¯
                if parquet_files:
                    first_file = parquet_files[0]
                    try:
                        scenario = scenario_serialization.load_argoverse_scenario_parquet(first_file)
                        
                        results['details']['sample_scenario'] = {
                            'scenario_id': scenario.scenario_id,
                            'track_count': len(scenario.tracks),
                            'timestep_count': len(scenario.tracks[0].object_states) if scenario.tracks else 0,
                            'map_id': scenario.map_id
                        }
                        
                    except Exception as e:
                        results['details']['scenario_load_error'] = str(e)
                
            except ImportError:
                results['details']['av2_api_available'] = False
                results['details']['av2_api_error'] = 'av2 package not installed'
            except Exception as e:
                results['details']['av2_api_available'] = False
                results['details']['av2_api_error'] = str(e)
                
        except Exception as e:
            results['status'] = 'error'
            results['details']['error'] = str(e)
        
        return results

def validate_all_datasets(base_data_dir: str) -> Dict:
    """éªŒè¯æ‰€æœ‰æ•°æ®é›†"""
    logger.info(f"å¼€å§‹éªŒè¯æ•°æ®é›†ï¼Œæ ¹ç›®å½•: {base_data_dir}")
    
    base_path = Path(base_data_dir)
    all_results = {
        'validation_time': subprocess.check_output(['date'], text=True).strip(),
        'base_directory': str(base_path),
        'datasets': {}
    }
    
    # éªŒè¯ nuScenes
    nuscenes_path = base_path / 'nuscenes'
    if nuscenes_path.exists():
        validator = NuScenesValidator(str(nuscenes_path))
        all_results['datasets']['nuscenes'] = validator.validate()
    else:
        all_results['datasets']['nuscenes'] = {
            'dataset': 'nuScenes',
            'status': 'not_found',
            'details': {'error': f'Directory not found: {nuscenes_path}'}
        }
    
    # éªŒè¯ Waymo
    waymo_path = base_path / 'waymo'
    if waymo_path.exists():
        validator = WaymoValidator(str(waymo_path))
        all_results['datasets']['waymo'] = validator.validate()
    else:
        all_results['datasets']['waymo'] = {
            'dataset': 'Waymo',
            'status': 'not_found',
            'details': {'error': f'Directory not found: {waymo_path}'}
        }
    
    # éªŒè¯ Argoverse 2
    argoverse_path = base_path / 'argoverse2'
    if argoverse_path.exists():
        validator = ArgoverseValidator(str(argoverse_path))
        all_results['datasets']['argoverse2'] = validator.validate()
    else:
        all_results['datasets']['argoverse2'] = {
            'dataset': 'Argoverse2',
            'status': 'not_found',
            'details': {'error': f'Directory not found: {argoverse_path}'}
        }
    
    return all_results

def print_validation_summary(results: Dict):
    """æ‰“å°éªŒè¯ç»“æœæ‘˜è¦"""
    print("\n" + "="*80)
    print("ğŸ“Š æ•°æ®é›†éªŒè¯ç»“æœæ‘˜è¦")
    print("="*80)
    
    total_size_mb = 0
    valid_datasets = 0
    
    for dataset_name, dataset_result in results['datasets'].items():
        status = dataset_result['status']
        details = dataset_result.get('details', {})
        
        # çŠ¶æ€å›¾æ ‡
        if status == 'valid':
            icon = "âœ…"
            valid_datasets += 1
        elif status == 'incomplete':
            icon = "âš ï¸"
        elif status == 'not_found':
            icon = "âŒ"
        else:
            icon = "â“"
        
        print(f"\n{icon} {dataset_result['dataset']}:")
        print(f"   çŠ¶æ€: {status}")
        
        if 'dataset_size_mb' in details:
            size_mb = details['dataset_size_mb']
            total_size_mb += size_mb
            print(f"   å¤§å°: {size_mb:.1f} MB")
        
        # æ•°æ®é›†ç‰¹å®šä¿¡æ¯
        if dataset_name == 'nuscenes':
            if 'scene_count' in details:
                print(f"   åœºæ™¯æ•°: {details['scene_count']}")
            if 'sample_count' in details:
                print(f"   æ ·æœ¬æ•°: {details['sample_count']}")
            if 'api_available' in details:
                api_status = "âœ…" if details['api_available'] else "âŒ"
                print(f"   API: {api_status}")
        
        elif dataset_name == 'waymo':
            if 'tfrecord_count' in details:
                print(f"   TFRecord æ–‡ä»¶æ•°: {details['tfrecord_count']}")
            if 'tensorflow_available' in details:
                tf_status = "âœ…" if details['tensorflow_available'] else "âŒ"
                print(f"   TensorFlow: {tf_status}")
            if 'waymo_api_available' in details:
                api_status = "âœ…" if details['waymo_api_available'] else "âŒ"
                print(f"   Waymo API: {api_status}")
        
        elif dataset_name == 'argoverse2':
            if 'scenario_count' in details:
                print(f"   åœºæ™¯æ–‡ä»¶æ•°: {details['scenario_count']}")
            if 'av2_api_available' in details:
                api_status = "âœ…" if details['av2_api_available'] else "âŒ"
                print(f"   AV2 API: {api_status}")
        
        # é”™è¯¯ä¿¡æ¯
        if 'error' in details:
            print(f"   âŒ é”™è¯¯: {details['error']}")
    
    print(f"\nğŸ“Š æ€»ç»“:")
    print(f"   æœ‰æ•ˆæ•°æ®é›†: {valid_datasets}/3")
    print(f"   æ€»å¤§å°: {total_size_mb:.1f} MB ({total_size_mb/1024:.1f} GB)")
    print(f"   éªŒè¯æ—¶é—´: {results['validation_time']}")

def generate_config_file(results: Dict, output_path: str):
    """ç”Ÿæˆæ¨¡å‹é…ç½®æ–‡ä»¶"""
    config = {
        'datasets': {},
        'metadata': {
            'generated_at': results['validation_time'],
            'base_directory': results['base_directory']
        }
    }
    
    for dataset_name, dataset_result in results['datasets'].items():
        if dataset_result['status'] == 'valid':
            if dataset_name == 'nuscenes':
                config['datasets']['nuscenes'] = {
                    'dataroot': dataset_result['root_path'],
                    'version': 'v1.0-mini',
                    'available': True
                }
            elif dataset_name == 'waymo':
                config['datasets']['waymo'] = {
                    'dataroot': dataset_result['root_path'],
                    'data_split': 'validation',
                    'available': True
                }
            elif dataset_name == 'argoverse2':
                config['datasets']['argoverse2'] = {
                    'dataroot': dataset_result['root_path'],
                    'split': 'val',
                    'available': True
                }
        else:
            config['datasets'][dataset_name] = {'available': False}
    
    with open(output_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    logger.info(f"é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='éªŒè¯è‡ªåŠ¨é©¾é©¶æ•°æ®é›†')
    parser.add_argument('--data-dir', type=str, default='/data/datasets',
                       help='æ•°æ®é›†æ ¹ç›®å½• (é»˜è®¤: /data/datasets)')
    parser.add_argument('--output', type=str, default='validation_results.json',
                       help='éªŒè¯ç»“æœè¾“å‡ºæ–‡ä»¶ (é»˜è®¤: validation_results.json)')
    parser.add_argument('--config', type=str, default='dataset_config.json',
                       help='ç”Ÿæˆçš„é…ç½®æ–‡ä»¶ (é»˜è®¤: dataset_config.json)')
    parser.add_argument('--quiet', action='store_true',
                       help='é™é»˜æ¨¡å¼ï¼Œä»…è¾“å‡ºé”™è¯¯')
    
    args = parser.parse_args()
    
    if args.quiet:
        logging.getLogger().setLevel(logging.ERROR)
    
    try:
        # éªŒè¯æ•°æ®é›†
        results = validate_all_datasets(args.data_dir)
        
        # ä¿å­˜éªŒè¯ç»“æœ
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"éªŒè¯ç»“æœå·²ä¿å­˜: {args.output}")
        
        # ç”Ÿæˆé…ç½®æ–‡ä»¶
        generate_config_file(results, args.config)
        
        # æ‰“å°æ‘˜è¦
        if not args.quiet:
            print_validation_summary(results)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ•°æ®é›†
        valid_count = sum(1 for dataset in results['datasets'].values() 
                         if dataset['status'] == 'valid')
        
        if valid_count == 0:
            logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®é›†ï¼")
            sys.exit(1)
        else:
            logger.info(f"éªŒè¯å®Œæˆï¼Œæ‰¾åˆ° {valid_count} ä¸ªæœ‰æ•ˆæ•°æ®é›†")
            
    except Exception as e:
        logger.error(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()