# RunPod ç›´æ¥æ¨¡å‹æµ‹è¯•æŒ‡å—

## ğŸ¯ æ¦‚è¿°

è¿™ä¸ªæ–°æ–¹æ¡ˆè®©ä½ å¯ä»¥åœ¨RunPodç¯å¢ƒä¸­ç›´æ¥å®‰è£…å’Œæµ‹è¯•å¤šä¸ª3Dæ£€æµ‹å’Œåœ°å›¾æ„å»ºæ¨¡å‹ï¼Œæ— éœ€Dockeræ„å»ºã€‚æ”¯æŒåœ¨å•ä¸ªGPUå®ä¾‹ä¸Šæµ‹è¯•å¤šä¸ªæ¨¡å‹ï¼Œè¿›è¡Œæ€§èƒ½å¯¹æ¯”ã€‚

## ğŸš€ æ”¯æŒçš„æ¨¡å‹

- **MapTR** - åœ¨çº¿çŸ¢é‡åŒ–é«˜ç²¾åœ°å›¾æ„å»º
- **PETR** - å¤šè§†å›¾3Dç›®æ ‡æ£€æµ‹çš„ä½ç½®åµŒå…¥å˜æ¢
- **StreamPETR** - å¸¦æ—¶åºå»ºæ¨¡çš„é«˜æ•ˆå¤šè§†å›¾3Dç›®æ ‡æ£€æµ‹
- **TopoMLP** - è‡ªåŠ¨é©¾é©¶ä¸­æ‹“æ‰‘æ¨ç†çš„MLPæ¶æ„
- **VAD** - è‡ªåŠ¨é©¾é©¶çš„çŸ¢é‡åŒ–åœºæ™¯è¡¨ç¤º

## ğŸ“‹ RunPodå®ä¾‹è¦æ±‚

### æ¨èé…ç½®
- **GPU**: RTX 3090/4090 æˆ– A100 (24GB+ VRAM)
- **CPU**: 8+ æ ¸å¿ƒ
- **RAM**: 32GB+ ç³»ç»Ÿå†…å­˜
- **å­˜å‚¨**: 100GB+ SSD
- **é•œåƒ**: PyTorch/CUDA é•œåƒ (æ¨è `pytorch/pytorch:1.9.1-cuda11.1-cudnn8-devel`)

### æœ€ä½é…ç½®
- **GPU**: RTX 3080 (10GB VRAM)
- **CPU**: 4+ æ ¸å¿ƒ
- **RAM**: 16GB ç³»ç»Ÿå†…å­˜
- **å­˜å‚¨**: 50GB SSD

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨RunPodå®ä¾‹
```bash
# é€‰æ‹©åˆé€‚çš„PyTorch CUDAé•œåƒ
# ç¡®ä¿å¼€å¯Jupyterå’ŒSSHè®¿é—®
```

### 2. ä¸‹è½½è®¾ç½®è„šæœ¬
```bash
# åœ¨RunPodç»ˆç«¯ä¸­æ‰§è¡Œ
cd /workspace
git clone https://github.com/your-repo/online_mapping.git
cd online_mapping/runpod_docker
```

### 3. è¿è¡Œå®Œæ•´å®‰è£…
```bash
# å®Œæ•´å®‰è£…æ‰€æœ‰æ¨¡å‹å’Œä¾èµ–
./setup_runpod_environment.sh

# æˆ–è€…åˆ†æ­¥å®‰è£…
./setup_runpod_environment.sh base    # åŸºç¡€ç¯å¢ƒ
./setup_runpod_environment.sh models  # æ‰€æœ‰æ¨¡å‹
```

### 4. è®¾ç½®æ•°æ®
```bash
# è®¾ç½®ç¤ºä¾‹æ•°æ®
/workspace/setup_data.sh sample

# æˆ–è®¾ç½®NuScenesæ•°æ®
/workspace/setup_data.sh nuscenes
```

### 5. å¿«é€Ÿæµ‹è¯•
```bash
# æ¿€æ´»ç¯å¢ƒ
source /workspace/miniconda/bin/activate mapping_models

# è¿è¡Œå¿«é€Ÿæµ‹è¯•
/workspace/quick_test_models.sh
```

## ğŸ“Š è¯¦ç»†ä½¿ç”¨æ–¹æ³•

### ç¯å¢ƒç®¡ç†

```bash
# æ¿€æ´»æ¨¡å‹ç¯å¢ƒ
source /workspace/miniconda/bin/activate mapping_models

# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# æ£€æŸ¥PyTorch CUDA
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### å•ä¸ªæ¨¡å‹æµ‹è¯•

```bash
# ä½¿ç”¨ç»Ÿä¸€æµ‹è¯•æ¥å£
python /workspace/testing/model_tester.py \
  --model MapTR \
  --model-path /workspace/models/MapTR \
  --config /workspace/models/MapTR/projects/configs/maptrv2_nusc_r50_24e.py \
  --checkpoint /workspace/models/MapTR/checkpoints/maptrv2_nusc_r50_24e.pth \
  --data /workspace/data/sample \
  --output /workspace/test_results/maptrv2_results.json
```

### æ‰¹é‡æµ‹è¯•å¯¹æ¯”

```bash
# åˆ›å»ºæ‰¹é‡æµ‹è¯•è„šæœ¬
cat > /workspace/run_all_models.sh << 'EOF'
#!/bin/bash

source /workspace/miniconda/bin/activate mapping_models

models=("MapTR" "PETR" "StreamPETR" "TopoMLP" "VAD")
results_dir="/workspace/test_results"
data_path="/workspace/data/sample"

mkdir -p "$results_dir"

for model in "${models[@]}"; do
    echo "Testing $model..."
    python /workspace/testing/model_tester.py \
        --model "$model" \
        --model-path "/workspace/models/$model" \
        --config "/workspace/models/$model/configs/default.py" \
        --checkpoint "/workspace/models/$model/checkpoints/latest.pth" \
        --data "$data_path" \
        --output "$results_dir/${model}_results.json"
done

echo "All tests completed! Results in $results_dir"
EOF

chmod +x /workspace/run_all_models.sh
./run_all_models.sh
```

## ğŸ”§ é«˜çº§é…ç½®

### GPUå†…å­˜ç®¡ç†

```bash
# æ¸…ç†GPUå†…å­˜
python -c "import torch; torch.cuda.empty_cache()"

# ç›‘æ§GPUä½¿ç”¨
watch -n 1 nvidia-smi
```

### æ¨¡å‹åˆ‡æ¢

```bash
# åˆ‡æ¢åˆ°ç‰¹å®šæ¨¡å‹ç›®å½•
cd /workspace/models/MapTR

# è¿è¡Œæ¨¡å‹ç‰¹å®šè„šæœ¬
python tools/test.py configs/maptrv2_nusc_r50_24e.py checkpoints/maptrv2_nusc_r50_24e.pth
```

### è‡ªå®šä¹‰é…ç½®

```bash
# å¤åˆ¶å¹¶ä¿®æ”¹é…ç½®æ–‡ä»¶
cp /workspace/models/MapTR/configs/maptrv2_nusc_r50_24e.py \
   /workspace/configs/my_maptrv2_config.py

# ç¼–è¾‘é…ç½®
nano /workspace/configs/my_maptrv2_config.py
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹å¤„ç†å¤§å°è°ƒæ•´
```python
# åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒæ•´batch size
data = dict(
    samples_per_gpu=1,  # æ ¹æ®GPUå†…å­˜è°ƒæ•´
    workers_per_gpu=4,
)
```

### 2. ç²¾åº¦è®¾ç½®
```bash
# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
export CUDA_VISIBLE_DEVICES=0
python tools/test.py configs/config.py checkpoints/model.pth --fp16
```

### 3. å†…å­˜ä¼˜åŒ–
```python
# åœ¨Pythonè„šæœ¬ä¸­
import torch
torch.backends.cudnn.benchmark = True  # åŠ é€Ÿ
torch.cuda.empty_cache()  # æ¸…ç†å†…å­˜
```

## ğŸ—‚ï¸ ç›®å½•ç»“æ„

å®‰è£…å®Œæˆåçš„ç›®å½•ç»“æ„ï¼š

```
/workspace/
â”œâ”€â”€ miniconda/                 # Condaç¯å¢ƒ
â”œâ”€â”€ models/                    # æ‰€æœ‰æ¨¡å‹
â”‚   â”œâ”€â”€ MapTR/
â”‚   â”œâ”€â”€ PETR/
â”‚   â”œâ”€â”€ StreamPETR/
â”‚   â”œâ”€â”€ TopoMLP/
â”‚   â””â”€â”€ VAD/
â”œâ”€â”€ data/                      # æ•°æ®é›†
â”‚   â”œâ”€â”€ sample/
â”‚   â””â”€â”€ nuscenes/
â”œâ”€â”€ testing/                   # æµ‹è¯•å·¥å…·
â”‚   â””â”€â”€ model_tester.py
â”œâ”€â”€ test_results/              # æµ‹è¯•ç»“æœ
â”œâ”€â”€ configs/                   # è‡ªå®šä¹‰é…ç½®
â”œâ”€â”€ setup_data.sh              # æ•°æ®è®¾ç½®è„šæœ¬
â””â”€â”€ quick_test_models.sh       # å¿«é€Ÿæµ‹è¯•è„šæœ¬
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### GPUå†…å­˜ä¸è¶³
```bash
# è§£å†³æ–¹æ¡ˆ1: å‡å°‘batch size
# åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® samples_per_gpu=1

# è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
# åœ¨é…ç½®ä¸­è®¾ç½® accumulate_grad_batches=4

# è§£å†³æ–¹æ¡ˆ3: æ¸…ç†å†…å­˜
python -c "import torch; torch.cuda.empty_cache()"
```

#### ä¾èµ–å†²çª
```bash
# é‡æ–°åˆ›å»ºç¯å¢ƒ
conda remove -n mapping_models --all
conda create -n mapping_models python=3.8 -y
# é‡æ–°è¿è¡Œå®‰è£…è„šæœ¬
```

#### æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -la /workspace/models/MapTR/checkpoints/

# æ£€æŸ¥é…ç½®æ–‡ä»¶
python -c "from mmcv import Config; cfg = Config.fromfile('config.py'); print(cfg)"
```

### æ—¥å¿—å’Œè°ƒè¯•

```bash
# å¼€å¯è¯¦ç»†æ—¥å¿—
export PYTHONPATH=/workspace/models/MapTR:$PYTHONPATH
export MMDET_DEBUG=1

# ä¿å­˜æµ‹è¯•æ—¥å¿—
python test_script.py 2>&1 | tee test_log.txt
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### å…¸å‹æ€§èƒ½è¡¨ç° (RTX 3090)

| æ¨¡å‹ | æ¨ç†æ—¶é—´ | GPUå†…å­˜ä½¿ç”¨ | å‡†ç¡®ç‡ | ç‰¹ç‚¹ |
|------|----------|-------------|--------|------|
| MapTR | ~250ms | 2.1GB | 85% mAP | åœ¨çº¿åœ°å›¾æ„å»º |
| PETR | ~180ms | 1.8GB | 82% mAP | å¤šè§†å›¾æ£€æµ‹ |
| StreamPETR | ~220ms | 2.0GB | 84% mAP | æ—¶åºå»ºæ¨¡ |
| TopoMLP | ~150ms | 1.5GB | 78% mAP | æ‹“æ‰‘æ¨ç† |
| VAD | ~300ms | 2.3GB | 86% mAP | çŸ¢é‡åŒ–è¡¨ç¤º |

## ğŸ¯ ä½¿ç”¨æŠ€å·§

### 1. å¤šæ¨¡å‹æµ‹è¯•ç­–ç•¥
```bash
# ä¾æ¬¡æµ‹è¯•ï¼Œé¿å…å†…å­˜é—®é¢˜
for model in MapTR PETR StreamPETR; do
    python test_single_model.py --model $model
    python -c "import torch; torch.cuda.empty_cache()"
    sleep 5
done
```

### 2. ç»“æœåˆ†æ
```python
# åˆ†ææµ‹è¯•ç»“æœ
import json
import matplotlib.pyplot as plt

results = []
for model in ['MapTR', 'PETR', 'StreamPETR']:
    with open(f'/workspace/test_results/{model}_results.json') as f:
        results.append(json.load(f))

# ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾
models = [r['model_name'] for r in results]
times = [r['inference_time'] for r in results]
plt.bar(models, times)
plt.title('Model Inference Time Comparison')
plt.ylabel('Time (seconds)')
plt.savefig('/workspace/test_results/comparison.png')
```

### 3. è‡ªåŠ¨åŒ–æµ‹è¯•
```bash
# åˆ›å»ºcronå®šæ—¶ä»»åŠ¡
echo "0 */6 * * * /workspace/quick_test_models.sh" | crontab -
```

## ğŸ”„ ç»´æŠ¤å’Œæ›´æ–°

### æ›´æ–°æ¨¡å‹
```bash
cd /workspace/models/MapTR
git pull origin main

# é‡æ–°å®‰è£…ä¾èµ–
source /workspace/miniconda/bin/activate mapping_models
pip install -r requirements.txt
```

### ç¯å¢ƒå¤‡ä»½
```bash
# å¯¼å‡ºcondaç¯å¢ƒ
conda env export -n mapping_models > environment.yml

# æ¢å¤ç¯å¢ƒ
conda env create -f environment.yml
```

### æ•°æ®ç®¡ç†
```bash
# æ¸…ç†æ—§ç»“æœ
rm -rf /workspace/test_results/*

# å‹ç¼©æ•°æ®
tar -czf data_backup.tar.gz /workspace/data/
```

## ğŸ“ æ”¯æŒ

å¦‚é‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥ `/workspace/quick_test_models.sh` è¾“å‡º
2. æŸ¥çœ‹GPUçŠ¶æ€ `nvidia-smi`
3. æ£€æŸ¥Pythonç¯å¢ƒ `conda info --envs`
4. æŸ¥çœ‹é”™è¯¯æ—¥å¿— `tail -f /workspace/test_results/error.log`

---

**å®Œæˆå®‰è£…åï¼Œä½ å°†æ‹¥æœ‰ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„RunPodç¯å¢ƒï¼Œå¯ä»¥ç›´æ¥æµ‹è¯•å’Œå¯¹æ¯”å¤šä¸ª3Dæ£€æµ‹å’Œåœ°å›¾æ„å»ºæ¨¡å‹ï¼**