# å†…å­˜ä¼˜åŒ–æŒ‡å—

## ğŸ“Š å†…å­˜ä½¿ç”¨åˆ†æ

### å½“å‰çŠ¶å†µè¯„ä¼°
- **78%å†…å­˜ä½¿ç”¨ç‡åˆ†æ**: å¯¹äºAIæ¨¡å‹å®¹å™¨æ¥è¯´æ˜¯åé«˜ä½†å¯æ¥å—çš„æ°´å¹³
- **488MBæ€»å†…å­˜**: å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹æ¥è¯´åå°ï¼Œæ¨èè‡³å°‘1-2GB

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡
- é™ä½å†…å­˜ä½¿ç”¨ç‡åˆ°60%ä»¥ä¸‹
- æé«˜ç³»ç»Ÿç¨³å®šæ€§å’Œå“åº”é€Ÿåº¦
- é¢„é˜²å†…å­˜ä¸è¶³å¯¼è‡´çš„æ¨¡å‹åŠ è½½å¤±è´¥

## ğŸ› ï¸ ä¼˜åŒ–æ–¹æ¡ˆ

### 1. å®¹å™¨å†…å­˜é…ç½®ä¼˜åŒ–

#### RunPodéƒ¨ç½²æ¨èé…ç½®
```bash
# æ¨èå†…å­˜é…ç½®
--memory=2g --memory-swap=4g
```

#### Dockerè¿è¡Œé…ç½®
```bash
# åŸºç¡€é…ç½® (1GBå†…å­˜)
docker run --memory=1g --memory-swap=2g \
  iankaramazov/ai-models:maptr-latest

# æ¨èé…ç½® (2GBå†…å­˜)  
docker run --memory=2g --memory-swap=4g \
  --oom-kill-disable=false \
  iankaramazov/ai-models:maptr-latest

# é«˜æ€§èƒ½é…ç½® (4GBå†…å­˜)
docker run --memory=4g --memory-swap=8g \
  --oom-kill-disable=false \
  iankaramazov/ai-models:maptr-latest
```

### 2. è¿è¡Œæ—¶å†…å­˜ä¼˜åŒ–

#### ä½¿ç”¨å†…å­˜ä¼˜åŒ–å·¥å…·
```bash
# æ£€æŸ¥å†…å­˜çŠ¶æ€
python /app/tools/memory_optimizer.py --report

# æ‰§è¡Œå†…å­˜æ¸…ç†
python /app/tools/memory_optimizer.py --cleanup

# æŒç»­ç›‘æ§å†…å­˜
python /app/tools/memory_optimizer.py --monitor 60
```

#### æ¨¡å‹æ¨ç†æ—¶ä¼˜åŒ–
```python
# åœ¨æ¨¡å‹æ¨ç†å‰
from gpu_utils import cleanup_gpu_memory, monitor_memory_usage

# æ¸…ç†å†…å­˜
cleanup_gpu_memory()
monitor_memory_usage("before_inference")

# æ‰§è¡Œæ¨ç†
result = model.inference(data)

# æ¨ç†åæ¸…ç†
cleanup_gpu_memory()
monitor_memory_usage("after_inference")
```

### 3. ç³»ç»Ÿçº§ä¼˜åŒ–

#### ç¯å¢ƒå˜é‡é…ç½®
```bash
# æ·»åŠ åˆ°å®¹å™¨å¯åŠ¨ç¯å¢ƒå˜é‡
export PYTHONDONTWRITEBYTECODE=1  # ä¸ç”Ÿæˆ.pycæ–‡ä»¶
export PYTHONUNBUFFERED=1         # ä¸ç¼“å†²è¾“å‡º
export MALLOC_TRIM_THRESHOLD_=10000  # æ›´ç§¯æçš„å†…å­˜å›æ”¶
```

#### Pythonå†…å­˜ä¼˜åŒ–
```python
import gc
import sys

# æ›´æ¿€è¿›çš„åƒåœ¾å›æ”¶
gc.set_threshold(700, 10, 10)

# å®šæœŸæ‰‹åŠ¨GC
def periodic_cleanup():
    collected = gc.collect()
    print(f"å›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
```

### 4. æ¨¡å‹åŠ è½½ä¼˜åŒ–

#### æ‡’åŠ è½½ç­–ç•¥
```python
# åªåœ¨éœ€è¦æ—¶åŠ è½½æ¨¡å‹
class LazyModelLoader:
    def __init__(self):
        self._model = None
    
    @property
    def model(self):
        if self._model is None:
            self._model = self.load_model()
        return self._model
    
    def unload_model(self):
        del self._model
        self._model = None
        gc.collect()
```

## ğŸ“Š å†…å­˜ç›‘æ§æŒ‡æ ‡

### æ­£å¸¸è¿è¡ŒæŒ‡æ ‡
- **ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡**: < 70% (æ­£å¸¸), < 80% (å¯æ¥å—)
- **å¯ç”¨å†…å­˜**: > 200MB (æœ€ä½), > 500MB (æ¨è)
- **å†…å­˜ç¢ç‰‡**: å®šæœŸæ¸…ç†ï¼Œé¿å…é•¿æœŸè¿è¡Œå¯¼è‡´ç¢ç‰‡åŒ–

### å‘Šè­¦é˜ˆå€¼
- **ğŸŸ¡ è­¦å‘Š**: å†…å­˜ä½¿ç”¨ç‡ > 80%
- **ğŸŸ  ä¸¥é‡**: å†…å­˜ä½¿ç”¨ç‡ > 90%
- **ğŸ”´ å±é™©**: å¯ç”¨å†…å­˜ < 50MB

## ğŸš€ å¿«é€Ÿè¯Šæ–­å’Œè§£å†³

### é—®é¢˜è¯Šæ–­è„šæœ¬
```bash
#!/bin/bash
# å¿«é€Ÿå†…å­˜è¯Šæ–­

echo "=== å†…å­˜è¯Šæ–­æŠ¥å‘Š ==="
echo "å½“å‰æ—¶é—´: $(date)"
echo

# ç³»ç»Ÿå†…å­˜
free -h

# å®¹å™¨å†…å­˜é™åˆ¶
if [ -f /sys/fs/cgroup/memory/memory.limit_in_bytes ]; then
    echo "å®¹å™¨å†…å­˜é™åˆ¶: $(cat /sys/fs/cgroup/memory/memory.limit_in_bytes | numfmt --to=iec)"
fi

# è¿›ç¨‹å†…å­˜æ’åº
echo "å†…å­˜ä½¿ç”¨æœ€é«˜çš„è¿›ç¨‹:"
ps aux --sort=-%mem | head -6

# GPUå†…å­˜ (å¦‚æœæœ‰)
if command -v nvidia-smi &> /dev/null; then
    echo "GPUå†…å­˜:"
    nvidia-smi --query-gpu=memory.used,memory.total --format=csv,nounits,noheader
fi
```

### ç´§æ€¥å†…å­˜é‡Šæ”¾
```bash
# 1. æ¸…ç†Pythonç¼“å­˜
python -c "import gc; print(f'å›æ”¶: {gc.collect()}'); import torch; torch.cuda.empty_cache() if torch.cuda.is_available() else None"

# 2. æ¸…ç†ç³»ç»Ÿç¼“å­˜ (å¦‚æœæœ‰æƒé™)
sync && echo 3 > /proc/sys/vm/drop_caches 2>/dev/null || echo "éœ€è¦ç®¡ç†å‘˜æƒé™æ¸…ç†ç³»ç»Ÿç¼“å­˜"

# 3. å¼ºåˆ¶åƒåœ¾å›æ”¶
python /app/tools/memory_optimizer.py --cleanup
```

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

### å†…å­˜é…ç½®å»ºè®®

| æ¨¡å‹ç±»å‹ | æœ€å°å†…å­˜ | æ¨èå†…å­˜ | æœ€ä½³å†…å­˜ | è¯´æ˜ |
|---------|---------|---------|---------|------|
| MapTR   | 1GB     | 2GB     | 4GB     | åœ°å›¾æ„å»ºæ¨¡å‹ |
| PETR    | 1GB     | 2GB     | 4GB     | 3Dæ£€æµ‹æ¨¡å‹ |
| StreamPETR | 1.5GB | 3GB     | 6GB     | æ—¶åºæ¨¡å‹ï¼Œå†…å­˜éœ€æ±‚è¾ƒé«˜ |
| TopoMLP | 1GB     | 2GB     | 4GB     | æ‹“æ‰‘æ¨ç†æ¨¡å‹ |
| VAD     | 1.5GB   | 3GB     | 6GB     | åœºæ™¯è¡¨ç¤ºæ¨¡å‹ |

### ä¼˜åŒ–æ•ˆæœé¢„æœŸ
- **å†…å­˜ä½¿ç”¨ç‡**: 78% â†’ 50-60%
- **å“åº”æ—¶é—´**: æå‡20-30%
- **ç¨³å®šæ€§**: æ˜¾è‘—æ”¹å–„ï¼Œå‡å°‘OOMé”™è¯¯
- **å¹¶å‘èƒ½åŠ›**: æ”¯æŒæ›´å¤šåŒæ—¶æ¨ç†è¯·æ±‚

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. å†…å­˜ä½¿ç”¨ç‡æŒç»­é«˜äº90%
```bash
# è§£å†³æ–¹æ¡ˆ
docker restart <container_name>  # é‡å¯å®¹å™¨
# æˆ–å¢åŠ å†…å­˜é…ç½®
docker update --memory=2g <container_name>
```

#### 2. OOM (Out of Memory) é”™è¯¯
```bash
# æŸ¥çœ‹å†…å­˜é™åˆ¶
cat /sys/fs/cgroup/memory/memory.limit_in_bytes

# å¢åŠ swap
fallocate -l 2G /swapfile
chmod 600 /swapfile
mkswap /swapfile
swapon /swapfile
```

#### 3. å†…å­˜æ³„æ¼æ£€æµ‹
```python
import tracemalloc

# å¯åŠ¨å†…å­˜è·Ÿè¸ª
tracemalloc.start()

# è¿è¡Œä»£ç ...

# è·å–å†…å­˜ä½¿ç”¨ç»Ÿè®¡
current, peak = tracemalloc.get_traced_memory()
print(f"å½“å‰å†…å­˜: {current / 1024 / 1024:.1f}MB")
print(f"å³°å€¼å†…å­˜: {peak / 1024 / 1024:.1f}MB")
tracemalloc.stop()
```

## ğŸ“š ç›¸å…³æ–‡æ¡£
- [RunPodå†…å­˜é…ç½®æ–‡æ¡£](RUNPOD_SETUP_GUIDE.md)
- [æ¨¡å‹æ€§èƒ½ä¼˜åŒ–æŒ‡å—](../technical/PERFORMANCE_OPTIMIZATION.md)
- [GPUå†…å­˜ç®¡ç†](../technical/GPU_MEMORY_MANAGEMENT.md)